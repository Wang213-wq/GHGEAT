"""
GNN-Gibbs-Helmholtzæ¸©åº¦è®­ç»ƒ
"""
import copy
import math
import os
import random
import sys
import time
from collections import OrderedDict
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from rdkit import Chem
from torch.optim.lr_scheduler import (
    CosineAnnealingLR,
    CosineAnnealingWarmRestarts,
    CyclicLR,
    LambdaLR,
    ReduceLROnPlateau
)

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨å¯¼å…¥è·¯å¾„ä¸­ï¼ˆä½¿å¾— `utilities_v2` å¯è¢«æ‰¾åˆ°ï¼‰
PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from scr.models.GH_pyGEAT_architecture_0615_v0 import (
    GHGEAT,
    count_parameters
)
from scr.models.test_model_on_testset import evaluate_on_testset
from scr.models.utilities_v2.Train_eval import EarlyStopping
from scr.models.utilities_v2.Train_eval_T import MAE, R2, eval, train
from scr.models.utilities_v2.mol2graph import (
    get_dataloader_pairs_T,
    n_atom_features,
    n_bond_features,
    sys2graph
)
from scr.models.utilities_v2.save_info import save_train_traj


class FirstEpochMAEThresholdExceeded(Exception):
    """ç¬¬ä¸€è½®MAEè¶…è¿‡é˜ˆå€¼ï¼Œåº”æå‰ç»ˆæ­¢è¯•éªŒ"""
    def __init__(self, first_epoch_mae: float, threshold: float):
        self.first_epoch_mae = first_epoch_mae
        self.threshold = threshold
        super().__init__(
            f"ç¬¬ä¸€è½®MAE ({first_epoch_mae:.6f}) è¶…è¿‡é˜ˆå€¼ "
            f"({threshold:.6f})ï¼Œæå‰ç»ˆæ­¢è¯•éªŒ"
        )

def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1, eta_min=0.0):
    """
    åˆ›å»ºä¸€ä¸ªå¸¦warmupçš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Parameters:
    -----------
    optimizer : torch.optim.Optimizer
        ä¼˜åŒ–å™¨
    num_warmup_steps : int
        Warmupæ­¥æ•°ï¼ˆepochæ•°ï¼‰
    num_training_steps : int
        æ€»è®­ç»ƒæ­¥æ•°ï¼ˆepochæ•°ï¼‰
    num_cycles : float
        ä½™å¼¦å‘¨æœŸçš„æ•°é‡ï¼Œé»˜è®¤0.5ï¼ˆåŠå‘¨æœŸï¼‰
    last_epoch : int
        æœ€åä¸€ä¸ªepochçš„ç´¢å¼•ï¼Œé»˜è®¤-1
    eta_min : float
        æœ€å°å­¦ä¹ ç‡ï¼Œé»˜è®¤0.0
    
    Returns:
    --------
    torch.optim.lr_scheduler.LambdaLR
        å¸¦warmupçš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    # ä¼˜åŒ–ï¼šé¢„å…ˆè®¡ç®—åˆå§‹å­¦ä¹ ç‡ï¼Œé¿å…æ¯æ¬¡è°ƒç”¨æ—¶è®¿é—®optimizer.defaults
    initial_lr = optimizer.defaults['lr']
    
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢åŠ å­¦ä¹ ç‡
            return float(current_step) / float(max(1, num_warmup_steps))
        # Cosine annealingé˜¶æ®µ
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        # è®¡ç®—ä½™å¼¦é€€ç«å› å­ï¼Œè€ƒè™‘eta_min
        cosine_factor = 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))
        # å°†eta_minæ˜ å°„åˆ°å­¦ä¹ ç‡èŒƒå›´ï¼ˆä½¿ç”¨é¢„å…ˆè®¡ç®—çš„initial_lrï¼‰
        lr_scale = (1 - eta_min / initial_lr) * cosine_factor + eta_min / initial_lr
        return lr_scale
    
    return LambdaLR(optimizer, lr_lambda, last_epoch)


def mix_brouwer_data(df_train, brouwer_path='data/raw/Brouwer_2021.csv', mix_ratio=0.15, random_seed=42):
    """
    å°†Brouwer_2021æ•°æ®æ··åˆåˆ°è®­ç»ƒé›†ä¸­
    
    Parameters:
    -----------
    df_train : pd.DataFrame
        åŸå§‹è®­ç»ƒæ•°æ®
    brouwer_path : str
        Brouwer_2021æ•°æ®æ–‡ä»¶è·¯å¾„
    mix_ratio : float
        æ··åˆæ¯”ä¾‹ï¼Œé»˜è®¤0.15 (15%)ï¼ŒèŒƒå›´å»ºè®®0.1-0.2 (10%-20%)
    random_seed : int
        éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
    
    Returns:
    --------
    pd.DataFrame
        æ··åˆåçš„è®­ç»ƒæ•°æ®
    """
    # è®¾ç½®éšæœºç§å­
    random.seed(random_seed)
    np.random.seed(random_seed)
    
    # åŠ è½½Brouwer_2021æ•°æ®
    df_brouwer = pd.read_csv(brouwer_path)
    
    # è®¡ç®—éœ€è¦é‡‡æ ·çš„æ•°é‡
    n_brouwer = len(df_brouwer)
    n_sample = int(n_brouwer * mix_ratio)
    
    # éšæœºé‡‡æ ·
    sampled_indices = random.sample(range(n_brouwer), n_sample)
    df_brouwer_sampled = df_brouwer.iloc[sampled_indices].copy()
    
    # ç¡®ä¿åˆ—åä¸€è‡´ï¼ˆå¦‚æœéœ€è¦ï¼‰
    required_columns = ['Solute_SMILES', 'Solvent_SMILES', 'log-gamma', 'T']
    if not all(col in df_brouwer_sampled.columns for col in required_columns):
        raise ValueError(f"Brouwer_2021æ•°æ®ç¼ºå°‘å¿…éœ€çš„åˆ—: {required_columns}")
    
    # åˆå¹¶æ•°æ®
    df_mixed = pd.concat([df_train, df_brouwer_sampled], ignore_index=True)
    
    print(f"åŸå§‹è®­ç»ƒé›†å¤§å°: {len(df_train)}")
    print(f"Brouwer_2021æ€»å¤§å°: {n_brouwer}")
    print(f"é‡‡æ ·Brouwer_2021æ•°é‡: {n_sample} ({mix_ratio*100:.1f}%)")
    print(f"æ··åˆåè®­ç»ƒé›†å¤§å°: {len(df_mixed)}")
    
    return df_mixed

    
def train_GNNGH_T(df, model_name, hyperparameters, mix_brouwer_ratio=None, resume_checkpoint=None, start_epoch_override=None, val_df=None):
    """
    è®­ç»ƒGHGEATæ¨¡å‹
    
    Parameters:
    -----------
    df : pd.DataFrame
        è®­ç»ƒæ•°æ®
    model_name : str
        æ¨¡å‹åç§°ï¼ˆç”¨äºä¿å­˜è·¯å¾„ï¼‰
    hyperparameters : dict
        è¶…å‚æ•°å­—å…¸ï¼ŒåŒ…å« hidden_dim, lr, n_epochs, batch_size
    mix_brouwer_ratio : float or None, optional
        æ··åˆBrouwer_2021æ•°æ®çš„æ¯”ä¾‹ï¼Œé»˜è®¤Noneè¡¨ç¤ºä¸æ··åˆ
    resume_checkpoint : str or None, optional
        æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä»è¯¥æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œé»˜è®¤Noneè¡¨ç¤ºä»å¤´è®­ç»ƒ
    start_epoch_override : int or None, optional
        å¼ºåˆ¶ä»æŒ‡å®šè½®æ¬¡å¼€å§‹è®­ç»ƒï¼Œè¦†ç›–checkpointä¸­çš„epochä¿¡æ¯ï¼Œé»˜è®¤Noneè¡¨ç¤ºä½¿ç”¨checkpointä¸­çš„epoch
    val_df : pd.DataFrame or None, optional
        éªŒè¯æ•°æ®ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨éªŒè¯é›†è¿›è¡Œè¯„ä¼°å’Œæ—©åœï¼Œé»˜è®¤Noneè¡¨ç¤ºä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œè¯„ä¼°
    """
    def _to_epoch_set(value):
        if value is None:
            return set()
        if isinstance(value, (int, float)):
            return {int(value)}
        if isinstance(value, str):
            try:
                return {int(value)}
            except ValueError:
                return set()
        try:
            return {int(item) for item in value}
        except Exception:
            return set()

    scheduler_restart_epochs_relative = _to_epoch_set(hyperparameters.get('scheduler_restart_epochs_relative'))
    scheduler_restart_epochs_absolute = _to_epoch_set(hyperparameters.get('scheduler_restart_epochs_absolute'))
    use_cyclic_lr = hyperparameters.get('use_cyclic_lr', False)
    cyclic_lr_params = hyperparameters.get('cyclic_lr_params', {})
    use_cosine_warm_restarts = hyperparameters.get('use_cosine_warm_restarts', False)
    cosine_restart_params = hyperparameters.get('cosine_restart_params', {})
    scheduler_type = 'plateau'
    def _create_scheduler(optimizer):
        nonlocal scheduler_type
        current_lr = hyperparameters.get('lr', lr)
        if use_cyclic_lr:
            scheduler_type = 'cyclic'
            cyclic_defaults = {
                'base_lr': cyclic_lr_params.get('base_lr', current_lr),
                'max_lr': cyclic_lr_params.get('max_lr', current_lr * 3),
                'step_size_up': cyclic_lr_params.get('step_size_up', 10),
                'mode': cyclic_lr_params.get('mode', 'triangular2'),
                'cycle_momentum': cyclic_lr_params.get('cycle_momentum', False),
                'last_epoch': cyclic_lr_params.get('last_epoch', -1),
            }
            cyclic_defaults.update({k: v for k, v in cyclic_lr_params.items() if k not in cyclic_defaults})
            return CyclicLR(optimizer, **cyclic_defaults)
        if use_cosine_warm_restarts:
            scheduler_type = 'cosine_restart'
            restart_defaults = {
                'T_0': cosine_restart_params.get('T_0', 20),
                'T_mult': cosine_restart_params.get('T_mult', 2),
                'eta_min': cosine_restart_params.get('eta_min', 1e-6),
                'last_epoch': cosine_restart_params.get('last_epoch', -1),
            }
            restart_defaults.update({k: v for k, v in cosine_restart_params.items() if k not in restart_defaults})
            return CosineAnnealingWarmRestarts(optimizer, **restart_defaults)
        scheduler_type = 'plateau'
        return ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=10,
            min_lr=1e-7
        )
    
    def _reset_optimizer_momentum(optimizer):
        """æ¸…ç©º AdamW/momentum ç¼“å­˜ï¼Œè®©é‡å¯æ—¶é‡æ–°ç§¯ç´¯åŠ¨é‡"""
        for param_group in optimizer.param_groups:
            for p in param_group['params']:
                state = optimizer.state.get(p, {})
                state.pop('exp_avg', None)
                state.pop('exp_avg_sq', None)
                state.pop('max_exp_avg_sq', None)
                state.pop('momentum_buffer', None)
                state.pop('step', None)
                if state:
                    optimizer.state[p] = state

    checkpoint_interval = max(1, int(hyperparameters.get('checkpoint_interval', 1)))

    # æµ‹è¯•é›†è¯„ä¼°é…ç½®ï¼ˆä»…åœ¨è®­ç»ƒå®Œæˆåè¯„ä¼°ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
    test_eval_path = hyperparameters.get('test_eval_path')
    test_eval_subset = hyperparameters.get('test_eval_subset_size')
    test_eval_batch = int(hyperparameters.get('test_eval_batch_size', 64))
    test_df_cache = None

    # æ„å»ºä¿å­˜è·¯å¾„ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨ReLUç›®å½•ï¼ˆè¶…å‚æ•°æœç´¢æ¨¡å¼ï¼‰
    # è¶…å‚æ•°æœç´¢æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä¿å­˜åœ¨ ReLU/{model_name}/ ç›®å½•ä¸‹
    # å¦‚æœæä¾›äº†è‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼ˆå¾®è°ƒæ¨¡å¼ï¼‰ï¼Œä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„
    custom_save_path = hyperparameters.get('custom_save_path')
    checkpoint_save_dir = hyperparameters.get('checkpoint_save_dir')
    training_files_save_dir = hyperparameters.get('training_files_save_dir')
    
    if custom_save_path:
        # å¾®è°ƒæ¨¡å¼ï¼šä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„ç»“æ„
        path = custom_save_path
        if checkpoint_save_dir:
            checkpoint_dir = checkpoint_save_dir
        else:
            checkpoint_dir = os.path.join(path, 'checkpoint')
        if training_files_save_dir:
            training_files_dir = training_files_save_dir
        else:
            training_files_dir = os.path.join(path, 'Training_files')
    else:
        # å¸¸è§„æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨ReLUç›®å½•ï¼ˆè¶…å‚æ•°æœç´¢æ¨¡å¼ï¼‰
        relu_path = os.path.join(os.getcwd(), 'ReLU', model_name)
        normal_path = os.path.join(os.getcwd(), model_name)
        
        # ä¼˜å…ˆæ£€æŸ¥ReLUç›®å½•æ˜¯å¦å­˜åœ¨ï¼ˆè¶…å‚æ•°æœç´¢æ¨¡å¼ï¼‰
        # æˆ–è€…æ£€æŸ¥æ˜¯å¦å­˜åœ¨ReLUç›®å½•ä¸‹çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        checkpoint_in_relu = os.path.join(relu_path, 'checkpoint', f'{model_name}_checkpoint.pth')
        
        if resume_checkpoint and 'ReLU' in resume_checkpoint:
            # ä»æ£€æŸ¥ç‚¹è·¯å¾„æ¨æ–­ï¼šæå–ç›®å½•è·¯å¾„
            path = os.path.dirname(resume_checkpoint)
        elif os.path.exists(relu_path) or os.path.exists(checkpoint_in_relu):
            # ReLUç›®å½•å­˜åœ¨æˆ–æ£€æŸ¥ç‚¹å­˜åœ¨äºReLUç›®å½•ï¼šä½¿ç”¨ReLUè·¯å¾„ï¼ˆè¶…å‚æ•°æœç´¢æ¨¡å¼ï¼‰
            path = relu_path
        else:
            # å¸¸è§„æ¨¡å¼ï¼šä¿å­˜åˆ°å½“å‰ç›®å½•ä¸‹çš„ {model_name}/ ç›®å½•
            path = normal_path
        
        checkpoint_dir = os.path.join(path, 'checkpoint')
        training_files_dir = path  # å¸¸è§„æ¨¡å¼ä¸‹ï¼Œè®­ç»ƒæ–‡ä»¶ä¿å­˜åœ¨ä¸»ç›®å½•
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir, exist_ok=True)
    if training_files_dir and not os.path.exists(training_files_dir):
        os.makedirs(training_files_dir, exist_ok=True)
    
    checkpoint_path = os.path.join(checkpoint_dir, f'{model_name}_checkpoint.pth')

    # Open report fileï¼ˆä¿å­˜åˆ°è®­ç»ƒæ–‡ä»¶ç›®å½•ï¼‰
    report_path = training_files_dir if custom_save_path else path
    report = open(os.path.join(report_path, 'Report_training_' + model_name + '.txt'), 'w', encoding='utf-8')
    def print_report(string, file=report):
        print(string)
        file.write('\n' + string)

    print_report(' Report for ' + model_name)
    print_report('-'*50)
    
    # è®°å½•æ•°æ®ä¿¡æ¯
    print_report(f'Training data size: {len(df)}')
    if val_df is not None:
        print_report(f'Validation data size: {len(val_df)}')
    if mix_brouwer_ratio is not None:
        print_report(f'Brouwer_2021 mix ratio: {mix_brouwer_ratio*100:.1f}%')
    
    # Build molecule from SMILES
    mol_column_solvent     = 'Molecule_Solvent'
    df[mol_column_solvent] = df['Solvent_SMILES'].apply(Chem.MolFromSmiles)

    mol_column_solute      = 'Molecule_Solute'
    df[mol_column_solute]  = df['Solute_SMILES'].apply(Chem.MolFromSmiles)
    
    train_index = df.index.tolist()
    
    target = 'log-gamma'
    
    # targets = ['K1', 'K2']
    # scaler = MinMaxScaler()
    # scaler = scaler.fit(df[targets].to_numpy())
    scaler = None
    
    graphs_solv, graphs_solu = 'g_solv', 'g_solu'
    df[graphs_solv], df[graphs_solu] = sys2graph(df, mol_column_solvent, mol_column_solute, target, y_scaler=scaler)
    
    # Hyperparametersï¼ˆéœ€è¦åœ¨åˆ›å»ºéªŒè¯é›†dataloaderä¹‹å‰è·å–batch_sizeï¼‰
    hidden_dim        = hyperparameters['hidden_dim']
    lr                = hyperparameters['lr']
    n_epochs          = hyperparameters['n_epochs']
    batch_size        = hyperparameters['batch_size']
    
    # å¤„ç†éªŒè¯é›†ï¼ˆå¦‚æœæä¾›ï¼‰
    val_loader = None
    if val_df is not None:
        val_df = val_df.copy()
        val_df[mol_column_solvent] = val_df['Solvent_SMILES'].apply(Chem.MolFromSmiles)
        val_df[mol_column_solute] = val_df['Solute_SMILES'].apply(Chem.MolFromSmiles)
        val_index = val_df.index.tolist()
        val_df[graphs_solv], val_df[graphs_solu] = sys2graph(val_df, mol_column_solvent, mol_column_solute, target, y_scaler=scaler)
        
        # è·å–æ•°æ®åŠ è½½å‚æ•°ï¼ˆç¦ç”¨æ‰€æœ‰åŠ é€Ÿä¼˜åŒ–ï¼Œä½¿ç”¨æœ€ä¿å®ˆçš„é»˜è®¤å€¼ï¼‰
        num_workers = hyperparameters.get('num_workers', 0)  # å•è¿›ç¨‹æ¨¡å¼ï¼Œç¦ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
        pin_memory = hyperparameters.get('pin_memory', False)  # ç¦ç”¨å†…å­˜å›ºå®š
        persistent_workers = hyperparameters.get('persistent_workers', False)  # ç¦ç”¨æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
        prefetch_factor = hyperparameters.get('prefetch_factor', 2)  # æœ€å°é¢„å–å› å­
        
        val_loader = get_dataloader_pairs_T(val_df, 
                                            val_index, 
                                            graphs_solv,
                                            graphs_solu,
                                            batch_size, 
                                            shuffle=False, 
                                            drop_last=False,
                                            num_workers=num_workers,
                                            pin_memory=pin_memory,
                                            persistent_workers=persistent_workers,
                                            prefetch_factor=prefetch_factor)
        print_report(f'âœ“ éªŒè¯é›†å·²åŠ è½½ï¼Œå°†ä½¿ç”¨éªŒè¯é›†è¿›è¡Œè¯„ä¼°å’Œæ—©åœ')
    fine_tune_epochs  = hyperparameters.get('fine_tune_epochs')
    force_layered_lr  = hyperparameters.get('force_layered_lr', False)
    
    start       = time.time()
    
    # è·å–æ•°æ®åŠ è½½å‚æ•°ï¼ˆç¦ç”¨æ‰€æœ‰åŠ é€Ÿä¼˜åŒ–ï¼Œä½¿ç”¨æœ€ä¿å®ˆçš„é»˜è®¤å€¼ï¼‰
    num_workers = hyperparameters.get('num_workers', 0)  # å•è¿›ç¨‹æ¨¡å¼ï¼Œç¦ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ
    pin_memory = hyperparameters.get('pin_memory', False)  # ç¦ç”¨å†…å­˜å›ºå®š
    persistent_workers = hyperparameters.get('persistent_workers', False)  # ç¦ç”¨æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
    prefetch_factor = hyperparameters.get('prefetch_factor', 2)  # æœ€å°é¢„å–å› å­
    
    # Data loadersï¼ˆä¼˜åŒ–ç‰ˆï¼šå¤šè¿›ç¨‹åŠ è½½å’Œå†…å­˜å›ºå®šï¼‰
    train_loader = get_dataloader_pairs_T(df, 
                                          train_index, 
                                          graphs_solv,
                                          graphs_solu,
                                          batch_size, 
                                          shuffle=True, 
                                          drop_last=True,
                                          num_workers=num_workers,
                                          pin_memory=pin_memory,
                                          persistent_workers=persistent_workers,
                                          prefetch_factor=prefetch_factor)
    
    available_device = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(available_device)
    
    # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒçš„GradScaler
    from torch.amp import GradScaler
    use_mixed_precision = hyperparameters.get('use_mixed_precision', False)  # é»˜è®¤ç¦ç”¨æ··åˆç²¾åº¦ï¼ˆé¿å… GradScaler å…¼å®¹æ€§é—®é¢˜ï¼‰
    scaler = GradScaler('cuda') if (use_mixed_precision and torch.cuda.is_available()) else None
    if scaler is not None:
        print_report('âœ“ æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨ï¼ˆFP16/FP32æ··åˆï¼‰')
    else:
        if not torch.cuda.is_available():
            print_report('âš ï¸ æ··åˆç²¾åº¦è®­ç»ƒå·²ç¦ç”¨ï¼ˆCPUæ¨¡å¼ä¸æ”¯æŒï¼‰')
        else:
            print_report('âš ï¸ æ··åˆç²¾åº¦è®­ç»ƒå·²ç¦ç”¨ï¼ˆuse_mixed_precision=Falseï¼‰')
    
    # æ£€æŸ¥å¹¶æŠ¥å‘ŠCUDAä½¿ç”¨æƒ…å†µ
    print_report('='*60)
    print_report('ã€è®¾å¤‡æ£€æŸ¥ã€‘')
    print_report(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
    print_report(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
    if torch.cuda.is_available():
        print_report(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
        print_report(f'GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}')
        print_report(f'å½“å‰GPU: {torch.cuda.get_device_name(0)}')
        print_report(f'GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')
        print_report(f'ä½¿ç”¨è®¾å¤‡: {device}')
    else:
        print_report(f'âš ï¸ è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰')
        print_report(f'ä½¿ç”¨è®¾å¤‡: {device}')
    print_report('='*60)
    
    # Model
    v_in = n_atom_features()
    e_in = n_bond_features()
    u_in = 3 # ap, bp, topopsa
    # æ³¨æ„åŠ›ä½¿ç”¨æ¯”ä¾‹ï¼š1.0è¡¨ç¤ºå®Œå…¨ä½¿ç”¨æ³¨æ„åŠ›ï¼ˆåŸå§‹è¡Œä¸ºï¼‰ï¼Œ0.0-1.0ä¹‹é—´å¯ä»¥è°ƒæ•´
    attention_weight = hyperparameters.get('attention_weight', 1.0)
    model = GHGEAT(v_in, e_in, u_in, hidden_dim, attention_weight=attention_weight)
    model = model.to(device)
    
    # ğŸ”‘ è¯Šæ–­æ¨¡å‹å‚æ•°åˆå§‹åŒ–çŠ¶æ€ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„æƒé‡ï¼‰
    print_report('ã€æ¨¡å‹å‚æ•°è¯Šæ–­ã€‘æ£€æŸ¥æ¨¡å‹å‚æ•°èŒƒå›´...')
    try:
        param_stats = []
        for name, param in model.named_parameters():
            if param.requires_grad:
                param_min = param.data.min().item()
                param_max = param.data.max().item()
                param_mean = param.data.mean().item()
                param_std = param.data.std().item()
                param_stats.append({
                    'name': name,
                    'min': param_min,
                    'max': param_max,
                    'mean': param_mean,
                    'std': param_std
                })
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„æƒé‡ï¼ˆå¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼‰
                if abs(param_max) > 1000 or abs(param_min) > 1000:
                    print_report(f'  âš ï¸  {name}: èŒƒå›´=[{param_min:.2f}, {param_max:.2f}], å‡å€¼={param_mean:.2f}, æ ‡å‡†å·®={param_std:.2f}')
        
        # ç‰¹åˆ«æ£€æŸ¥è¾“å‡ºå±‚ï¼ˆmlp3aå’Œmlp3bï¼‰ï¼Œè¿™äº›å±‚çš„æƒé‡ç›´æ¥å½±å“é¢„æµ‹å€¼
        output_layers = ['mlp3a', 'mlp3b']
        for layer_name in output_layers:
            for name, param in model.named_parameters():
                if layer_name in name and 'weight' in name and param.requires_grad:
                    weight_max = param.data.abs().max().item()
                    if weight_max > 100:
                        print_report(f'  âš ï¸  è¾“å‡ºå±‚ {name} çš„æƒé‡ç»å¯¹å€¼è¾ƒå¤§: {weight_max:.2f}ï¼Œå¯èƒ½å½±å“æ•°å€¼ç¨³å®šæ€§')
        
        print_report('âœ“ æ¨¡å‹å‚æ•°è¯Šæ–­å®Œæˆ')
    except Exception as e:
        print_report(f'  âš ï¸  å‚æ•°è¯Šæ–­å¤±è´¥: {e}')
    
    # è¾“å‡ºæ³¨æ„åŠ›ä½¿ç”¨æ¯”ä¾‹ä¿¡æ¯
    if attention_weight != 1.0:
        print_report(f'æ³¨æ„åŠ›ä½¿ç”¨æ¯”ä¾‹: {attention_weight:.2f} (1.0=å®Œå…¨ä½¿ç”¨, 0.0=å®Œå…¨è·³è¿‡)')
    
    # PyTorch 2.0+ ç¼–è¯‘ä¼˜åŒ–ï¼ˆå¯é€‰ï¼Œå¯æ˜¾è‘—åŠ é€Ÿè®­ç»ƒï¼‰
    # é»˜è®¤ç¦ç”¨ï¼Œå› ä¸ºéœ€è¦ Triton ä¸”å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜
    use_torch_compile = hyperparameters.get('use_torch_compile', False)  # é»˜è®¤ç¦ç”¨ï¼ˆé¿å… Triton ä¾èµ–é—®é¢˜ï¼‰
    model_compiled = False
    if use_torch_compile and hasattr(torch, 'compile'):
        compile_mode = hyperparameters.get('torch_compile_mode', 'reduce-overhead')  # é»˜è®¤æ¨¡å¼
        print_report('')
        print_report('='*60)
        print_report('ã€æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–ã€‘')
        print_report('='*60)
        print_report(f'å°è¯•ä½¿ç”¨ torch.compile() ä¼˜åŒ–æ¨¡å‹ï¼ˆæ¨¡å¼: {compile_mode}ï¼‰')
        print_report('æ³¨æ„ï¼šé¦–æ¬¡è¿è¡Œä¼šè¿›è¡Œç¼–è¯‘ï¼Œå¯èƒ½éœ€è¦é¢å¤–æ—¶é—´ï¼ˆé€šå¸¸10-60ç§’ï¼‰')
        print_report('ç¼–è¯‘åçš„æ¨¡å‹è¿è¡Œé€Ÿåº¦ä¼šæ˜¾è‘—æå‡ï¼ˆé€šå¸¸20-50%åŠ é€Ÿï¼‰')
        print_report('='*60)
        print_report('')
        
        # æ£€æŸ¥ Triton æ˜¯å¦å¯ç”¨ï¼ˆæŸäº›æ¨¡å¼éœ€è¦ Tritonï¼‰
        triton_available = False
        try:
            import triton
            triton_available = True
            print_report('âœ“ Triton å·²å®‰è£…')
        except ImportError:
            print_report('âš ï¸ Triton æœªå®‰è£…ï¼ŒæŸäº›ç¼–è¯‘æ¨¡å¼å¯èƒ½ä¸å¯ç”¨')
            # å¦‚æœæ¨¡å¼éœ€è¦ Tritonï¼Œåˆ‡æ¢åˆ°ä¸éœ€è¦ Triton çš„æ¨¡å¼
            if compile_mode in ['reduce-overhead', 'max-autotune']:
                print_report(f'   æ¨¡å¼ {compile_mode} éœ€è¦ Tritonï¼Œå°†åˆ‡æ¢åˆ° "default" æ¨¡å¼')
                compile_mode = 'default'
        
        try:
            # å°è¯•ç¼–è¯‘æ¨¡å‹
            model = torch.compile(model, mode=compile_mode)
            model_compiled = True
            print_report(f'âœ“ æ¨¡å‹ç¼–è¯‘æˆåŠŸï¼ˆæ¨¡å¼: {compile_mode}ï¼‰')
        except Exception as e:
            error_msg = str(e)
            print_report(f'âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥: {error_msg}')
            
            # å¦‚æœæ˜¯ Triton ç›¸å…³é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ä¸éœ€è¦ Triton çš„æ¨¡å¼
            if 'triton' in error_msg.lower() or 'TritonMissing' in str(type(e)):
                print_report('   æ£€æµ‹åˆ° Triton ç›¸å…³é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ "default" æ¨¡å¼ï¼ˆä¸éœ€è¦ Tritonï¼‰')
                try:
                    model = torch.compile(model, mode='default')
                    model_compiled = True
                    print_report('âœ“ ä½¿ç”¨ "default" æ¨¡å¼ç¼–è¯‘æˆåŠŸ')
                except Exception as e2:
                    print_report(f'âš ï¸ ä½¿ç”¨ "default" æ¨¡å¼ä¹Ÿå¤±è´¥: {e2}')
                    print_report('   å°†ä½¿ç”¨æœªç¼–è¯‘çš„æ¨¡å‹ç»§ç»­è®­ç»ƒ')
            else:
                print_report('   å°†ä½¿ç”¨æœªç¼–è¯‘çš„æ¨¡å‹ç»§ç»­è®­ç»ƒ')
    elif use_torch_compile and not hasattr(torch, 'compile'):
        print_report('âš ï¸ torch.compile() ä¸å¯ç”¨ï¼ˆéœ€è¦ PyTorch 2.0+ï¼‰ï¼Œè·³è¿‡ç¼–è¯‘ä¼˜åŒ–')
    else:
        print_report('âš ï¸ torch.compile() å·²ç¦ç”¨ï¼ˆuse_torch_compile=Falseï¼‰')
    
    # éªŒè¯æ¨¡å‹ç¡®å®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆå·²ç¦ç”¨è¾“å‡ºï¼‰
    # if torch.cuda.is_available():
    #     next_param = next(model.parameters())
    #     actual_device = next_param.device
    #     if actual_device.type != 'cuda':
    #         print_report(f'âš ï¸ è­¦å‘Š: æ¨¡å‹å‚æ•°ä¸åœ¨CUDAè®¾å¤‡ä¸Šï¼å®é™…è®¾å¤‡: {actual_device}')
    #     else:
    #         print_report(f'âœ“ æ¨¡å‹å·²åŠ è½½åˆ°CUDAè®¾å¤‡: {actual_device}')
    
    print('    Number of model parameters: ', count_parameters(model))
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æŸ¥ç‚¹éœ€è¦æ¢å¤ï¼ˆä¼˜å…ˆçº§é«˜äºé¢„è®­ç»ƒæƒé‡ï¼‰
    # éœ€è¦åœ¨ä¼˜åŒ–å™¨åˆ›å»ºä¹‹å‰æ£€æŸ¥ï¼Œä»¥ä¾¿å†³å®šæ˜¯å¦ä½¿ç”¨å¾®è°ƒå­¦ä¹ ç‡
    checkpoint_to_load = resume_checkpoint if resume_checkpoint else checkpoint_path
    has_checkpoint = os.path.exists(checkpoint_to_load)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯.pthæ–‡ä»¶ï¼ˆåªåŒ…å«æ¨¡å‹æƒé‡ï¼Œä¸åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ç­‰ï¼‰
    is_pth_file = False
    if has_checkpoint and checkpoint_to_load.endswith('.pth') and not checkpoint_to_load.endswith('_checkpoint.pth'):
        is_pth_file = True
        print_report(f'æ£€æµ‹åˆ°.pthæ–‡ä»¶ï¼ˆä»…æ¨¡å‹æƒé‡ï¼‰ï¼Œå°†ä»è¯¥æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡')
        print_report(f'æ³¨æ„ï¼š.pthæ–‡ä»¶ä¸åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€å’Œè®­ç»ƒå†å²ï¼Œè®­ç»ƒå°†ä»ç¬¬0è½®å¼€å§‹')
    
    # To save trajectory
    mae_train = []
    r2_train = []
    mae_valid = []  # éªŒè¯é›†MAEå†å²ï¼ˆå¦‚æœæä¾›äº†éªŒè¯é›†ï¼‰
    r2_valid = []   # éªŒè¯é›†RÂ²å†å²ï¼ˆå¦‚æœæä¾›äº†éªŒè¯é›†ï¼‰
    best_MAE = np.inf
    best_model = None
    start_epoch = 0
    
    # åˆå§‹åŒ–å¾®è°ƒç­–ç•¥å˜é‡ï¼ˆåœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡åè®¾ç½®ï¼‰
    use_pretrained = hyperparameters.get('use_pretrained', False)
    fine_tune_stage = hyperparameters.get('fine_tune_stage', 'two_stage')  # 'output_only', 'full', 'two_stage'
    freeze_shared_layers = False
    freeze_epochs = 0
    original_lr = lr  # ä¿å­˜åŸå§‹å­¦ä¹ ç‡
    
    # æ™ºèƒ½å­¦ä¹ ç‡é™ä½ç­–ç•¥ï¼šæ ¹æ®åŸå§‹å­¦ä¹ ç‡å¤§å°åŠ¨æ€è°ƒæ•´é™ä½å€æ•°
    def get_fine_tune_lr_reduction_factor(base_lr):
        """
        æ ¹æ®åŸºç¡€å­¦ä¹ ç‡è¿”å›åˆé€‚çš„é™ä½å€æ•°
        
        ç­–ç•¥ï¼š
        - å¦‚æœå­¦ä¹ ç‡ >= 1e-3: é™ä½10å€ï¼ˆ0.1ï¼‰
        - å¦‚æœå­¦ä¹ ç‡ >= 1e-4: é™ä½5å€ï¼ˆ0.2ï¼‰
        - å¦‚æœå­¦ä¹ ç‡ >= 1e-5: é™ä½3å€ï¼ˆ0.33ï¼‰
        - å¦‚æœå­¦ä¹ ç‡ < 1e-5: é™ä½2å€ï¼ˆ0.5ï¼‰
        """
        if base_lr >= 1e-3:
            return 0.1  # é™ä½10å€
        elif base_lr >= 1e-4:
            return 0.2  # é™ä½5å€
        elif base_lr >= 1e-5:
            return 0.33  # é™ä½3å€ï¼ˆçº¦ï¼‰
        else:
            return 0.5  # é™ä½2å€
    
    fine_tune_lr_factor = get_fine_tune_lr_reduction_factor(original_lr)
    task_type = 'regression'
    
    # åˆå§‹åŒ–æ£€æŸ¥ç‚¹çŠ¶æ€å˜é‡ï¼ˆå¦‚æœæ£€æŸ¥ç‚¹ä¸å­˜åœ¨ï¼Œè¿™äº›å˜é‡ä¸ºNoneï¼‰
    optimizer_state_dict = None
    scheduler_state_dict = None
    checkpoint_uses_layered_lr = False  # æ ‡è®°æ£€æŸ¥ç‚¹æ˜¯å¦ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡
    resumed_from_checkpoint = False  # æ ‡è®°æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
    resumed_best_mae = None  # è®°å½•æ¢å¤æ—¶çš„æœ€ä½³MAE
    lr_reduced = False  # æ ‡è®°æ˜¯å¦å·²ç»é™ä½è¿‡å­¦ä¹ ç‡ï¼ˆç”¨äºä».pthæ–‡ä»¶åŠ è½½æ—¶ï¼‰
    
    # æ³¨æ„ï¼šoptimizer å’Œ scheduler å°†åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡ååˆ›å»º
    # å› ä¸ºéœ€è¦æ ¹æ®æ˜¯å¦å†»ç»“å±‚æ¥å†³å®šä¼˜åŒ–å™¨çš„å‚æ•°
    
    # Early stopping mechanism
    early_stop_requires_best_update = hyperparameters.get('early_stop_requires_best_update', False)
    early_stop_resume_patience = hyperparameters.get('early_stopping_patience', hyperparameters.get('patience', 25))
    early_stop_min_delta = hyperparameters.get('early_stopping_min_delta', 1e-4)
    early_stop_pause_patience = hyperparameters.get('early_stopping_pause_patience', 999999)
    early_stopper = EarlyStopping(patience=early_stop_resume_patience, min_delta=early_stop_min_delta)
    early_stop_active = not early_stop_requires_best_update
    print_report(f'æ—©åœæœºåˆ¶: patience={early_stop_resume_patience}, min_delta={early_stop_min_delta}')
    
    # å¦‚æœæ²¡æœ‰æ£€æŸ¥ç‚¹ï¼Œå°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼ˆé€‰æ‹©æ€§åŠ è½½ï¼Œé¿å…è´Ÿè¿ç§»ï¼‰
    if not has_checkpoint:
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼ˆPyTorch 2.6 éœ€è¦è®¾ç½® weights_only=False æ¥åŠ è½½åŒ…å« sklearn å¯¹è±¡çš„æ–‡ä»¶ï¼‰
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            'hyperparameter_research/GHGEAT_Ki_search/GHGEAT_Ki_best/GHGEAT_Ki_best.pth',  # ä»é¡¹ç›®æ ¹ç›®å½•
            'scr/models/hyperparameter_research/GHGEAT_Ki_search/GHGEAT_Ki_best/GHGEAT_Ki_best.pth',  # æ—§è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
            os.path.join(os.path.dirname(__file__), '..', '..', 'hyperparameter_research', 'GHGEAT_Ki_search', 'GHGEAT_Ki_best', 'GHGEAT_Ki_best.pth'),  # ä»å½“å‰æ–‡ä»¶ä½ç½®
        ]
        
        pretrained_path = None
        for path in possible_paths:
            if os.path.exists(path):
                pretrained_path = path
                break
        
        # å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè·¯å¾„ï¼ˆç”¨äºé”™è¯¯æç¤ºï¼‰
        if pretrained_path is None:
            pretrained_path = possible_paths[0]
        
        use_pretrained = hyperparameters.get('use_pretrained', False)  # é»˜è®¤ç¦ç”¨é¢„è®­ç»ƒæƒé‡
        
        if use_pretrained and os.path.exists(pretrained_path):
            try:
                checkpoint = torch.load(pretrained_path, 
                                       map_location=torch.device(available_device), 
                                       weights_only=False)
                
                # è·å–é¢„è®­ç»ƒæƒé‡å­—å…¸
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    pretrained_dict = checkpoint['model_state_dict']
                else:
                    pretrained_dict = checkpoint
                
                # æ™ºèƒ½æƒé‡åŠ è½½ï¼šåŠ è½½æ‰€æœ‰å…¼å®¹çš„æƒé‡ï¼ˆå…±äº«å±‚ + è¾“å‡ºå±‚ï¼‰
                # æ¨¡ä»¿GHGNNçš„æˆåŠŸç­–ç•¥ï¼šä½¿ç”¨strict=Falseè‡ªåŠ¨åŒ¹é…æ‰€æœ‰å…¼å®¹å±‚
                # GHGNNé€šè¿‡åŠ è½½æ‰€æœ‰å±‚ï¼ˆåŒ…æ‹¬è¾“å‡ºå±‚ï¼‰å®ç°äº†MAEä»0.096é™åˆ°0.041çš„æ˜¾è‘—æå‡
                # è¾“å‡ºå±‚æƒé‡æä¾›æ›´å¥½çš„åˆå§‹åŒ–ï¼Œå³ä½¿ä»»åŠ¡ä¸å®Œå…¨åŒ¹é…ä¹Ÿèƒ½å¿«é€Ÿé€‚åº”
                model_dict = model.state_dict()
                matched_keys = []
                skipped_keys = []
                
                # ç»Ÿè®¡åŠ è½½çš„å±‚ç±»å‹
                shared_layer_count = 0
                task_a_count = 0
                task_b_count = 0
                other_count = 0
                
                # è°ƒè¯•ï¼šè¾“å‡ºé¢„è®­ç»ƒæ¨¡å‹çš„é”®åç¤ºä¾‹ï¼ˆä»…åœ¨é¦–æ¬¡åŠ è½½æ—¶è¾“å‡ºï¼Œé¿å…é‡å¤ï¼‰
                print_report(f'  é¢„è®­ç»ƒæ¨¡å‹é”®åæ€»æ•°: {len(pretrained_dict.keys())}')
                pretrained_key_samples = list(pretrained_dict.keys())[:10]
                print_report(f'  é¢„è®­ç»ƒæ¨¡å‹é”®åç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:')
                for sample_key in pretrained_key_samples:
                    print_report(f'    - {sample_key}')
                
                # è°ƒè¯•ï¼šè¾“å‡ºå½“å‰æ¨¡å‹çš„é”®åç¤ºä¾‹
                model_key_samples = list(model_dict.keys())[:10]
                print_report(f'  å½“å‰æ¨¡å‹é”®åç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:')
                for sample_key in model_key_samples:
                    print_report(f'    - {sample_key}')
                
                for key in pretrained_dict.keys():
                    model_key = None
                    layer_type = None
                    original_key = key
                    
                    # å¤„ç†shared_layerå‰ç¼€ï¼ˆKiæ¨¡å‹ä½¿ç”¨MTLæ¶æ„ï¼‰
                    # è¿™äº›å±‚å­¦ä¹ çš„æ˜¯é€šç”¨çš„åˆ†å­ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨Kié¢„æµ‹å’ŒTé¢„æµ‹ä¹‹é—´é«˜åº¦å¯è¿ç§»
                    if 'shared_layer.' in key:
                        model_key = key.replace('shared_layer.', '')
                        layer_type = 'shared'
                    # å¤„ç†Task_Aè¾“å‡ºå±‚ -> mlp1a/mlp2a/mlp3a
                    # K1å’Œå‚æ•°Aæœ‰ä¸€å®šç›¸å…³æ€§ï¼Œè¾“å‡ºå±‚æƒé‡å¯èƒ½æä¾›æ›´å¥½çš„åˆå§‹åŒ–
                    elif 'task_A.' in key:
                        model_key = key.replace('task_A.', '')
                        layer_type = 'task_A'
                    # å¤„ç†Task_Bè¾“å‡ºå±‚ -> mlp1b/mlp2b/mlp3b
                    # è™½ç„¶K2å’Œå‚æ•°Bç›¸å…³æ€§è¾ƒå¼±ï¼Œä½†è¾“å‡ºå±‚æƒé‡ä»å¯èƒ½æä¾›æœ‰ç”¨çš„åˆå§‹åŒ–
                    elif 'task_B.' in key:
                        model_key = key.replace('task_B.', '')
                        layer_type = 'task_B'
                    else:
                        # å…¶ä»–æœªè¯†åˆ«çš„é”®ï¼Œå°è¯•ç›´æ¥åŒ¹é…
                        model_key = key
                        layer_type = 'other'
                    
                    # æ£€æŸ¥æ˜¯å¦å¯ä»¥åŠ è½½
                    if model_key and model_key in model_dict:
                        if model_dict[model_key].shape == pretrained_dict[key].shape:
                            model_dict[model_key] = pretrained_dict[key]
                            matched_keys.append(f"{key} -> {model_key}")
                            # ç»Ÿè®¡å„ç±»å‹å±‚çš„æ•°é‡
                            if layer_type == 'shared':
                                shared_layer_count += 1
                            elif layer_type == 'task_A':
                                task_a_count += 1
                            elif layer_type == 'task_B':
                                task_b_count += 1
                            else:
                                other_count += 1
                        else:
                            skipped_keys.append(f"Shape mismatch: {key} (é¢„è®­ç»ƒ: {pretrained_dict[key].shape}) vs {model_key} (å½“å‰æ¨¡å‹: {model_dict[model_key].shape})")
                    elif model_key:
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šé€šè¿‡é”®åç»“æ„åŒ¹é…
                        found_alternative = False
                        
                        # æ–¹æ³•1ï¼šå°è¯•é€šè¿‡é”®ååç¼€åŒ¹é…ï¼ˆä¾‹å¦‚ï¼šgraphnet1.node_model.ext_attention.Mkï¼‰
                        pretrained_key_parts = key.split('.')
                        pretrained_suffix = pretrained_key_parts[-1] if len(pretrained_key_parts) > 0 else key
                        
                        # å¦‚æœé¢„è®­ç»ƒé”®æœ‰å¤šä¸ªéƒ¨åˆ†ï¼Œå°è¯•åŒ¹é…ç»“æ„
                        if len(pretrained_key_parts) > 1:
                            # å°è¯•åŒ¹é…ï¼šç§»é™¤ shared_layer/task_A/task_B å‰ç¼€åçš„ç»“æ„
                            pretrained_structure = '.'.join(pretrained_key_parts[1:])  # è·³è¿‡ç¬¬ä¸€ä¸ªå‰ç¼€
                            
                            for model_key_candidate in model_dict.keys():
                                model_key_parts = model_key_candidate.split('.')
                                # æ£€æŸ¥ç»“æ„æ˜¯å¦åŒ¹é…ï¼ˆä»ç¬¬äºŒä¸ªéƒ¨åˆ†å¼€å§‹ï¼‰
                                if len(model_key_parts) >= len(pretrained_key_parts) - 1:
                                    model_structure = '.'.join(model_key_parts[-(len(pretrained_key_parts)-1):])
                                    if model_structure == pretrained_structure:
                                        if model_dict[model_key_candidate].shape == pretrained_dict[key].shape:
                                            model_dict[model_key_candidate] = pretrained_dict[key]
                                            matched_keys.append(f"{key} -> {model_key_candidate} (ç»“æ„åŒ¹é…)")
                                            found_alternative = True
                                            if layer_type == 'shared':
                                                shared_layer_count += 1
                                            elif layer_type == 'task_A':
                                                task_a_count += 1
                                            elif layer_type == 'task_B':
                                                task_b_count += 1
                                            else:
                                                other_count += 1
                                            break
                        
                        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•é€šè¿‡æœ€åéƒ¨åˆ†ï¼ˆå±‚åï¼‰åŒ¹é…
                        if not found_alternative:
                            for model_key_candidate in model_dict.keys():
                                model_key_parts = model_key_candidate.split('.')
                                model_suffix = model_key_parts[-1] if len(model_key_parts) > 0 else model_key_candidate
                                
                                # æ£€æŸ¥åç¼€æ˜¯å¦ç›¸åŒä¸”å½¢çŠ¶åŒ¹é…
                                if pretrained_suffix == model_suffix and model_dict[model_key_candidate].shape == pretrained_dict[key].shape:
                                    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿é”®åç»“æ„ç›¸ä¼¼ï¼ˆä¾‹å¦‚éƒ½åŒ…å« graphnet1 æˆ– gnorm1ï¼‰
                                    pretrained_middle = '.'.join(pretrained_key_parts[1:-1]) if len(pretrained_key_parts) > 2 else ''
                                    model_middle = '.'.join(model_key_parts[1:-1]) if len(model_key_parts) > 2 else ''
                                    
                                    # å¦‚æœä¸­é—´éƒ¨åˆ†åŒ¹é…æˆ–éƒ½ä¸ºç©ºï¼Œåˆ™è®¤ä¸ºæ˜¯åŒ¹é…çš„
                                    if pretrained_middle == model_middle or (not pretrained_middle and not model_middle):
                                        model_dict[model_key_candidate] = pretrained_dict[key]
                                        matched_keys.append(f"{key} -> {model_key_candidate} (åç¼€åŒ¹é…)")
                                        found_alternative = True
                                        if layer_type == 'shared':
                                            shared_layer_count += 1
                                        elif layer_type == 'task_A':
                                            task_a_count += 1
                                        elif layer_type == 'task_B':
                                            task_b_count += 1
                                        else:
                                            other_count += 1
                                        break
                        
                        if not found_alternative:
                            skipped_keys.append(f"Key not found in model: {model_key} (åŸå§‹é”®: {original_key})")
                
                model.load_state_dict(model_dict, strict=False)
                
                # ä¸ºæ³¨æ„åŠ›æœºåˆ¶å±‚è®¾ç½®æ›´å¥½çš„æƒé‡åˆå§‹åŒ–ï¼ˆå› ä¸ºæ— æ³•ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼‰
                # GHGEATçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆExternalAttentionLayerï¼‰åœ¨MTLç‰ˆæœ¬ä¸­ä¸å­˜åœ¨æˆ–å±‚åä¸åŒ¹é…
                # ä½¿ç”¨Xavieråˆå§‹åŒ–å¯ä»¥æ›´å¥½åœ°é…åˆé¢„è®­ç»ƒçš„ç‰¹å¾æå–å±‚
                attention_init_count = 0
                for name, param in model.named_parameters():
                    if 'ext_attention' in name and ('Mk' in name or 'Mv' in name):
                        # ä½¿ç”¨Xavierå‡åŒ€åˆå§‹åŒ–ï¼Œé€‚åˆæ³¨æ„åŠ›æœºåˆ¶
                        # è¿™æœ‰åŠ©äºæ³¨æ„åŠ›å±‚æ›´å¥½åœ°ä¸é¢„è®­ç»ƒçš„ç‰¹å¾æå–å±‚é…åˆ
                        nn.init.xavier_uniform_(param, gain=1.0)
                        attention_init_count += 1
                
                print_report(f'å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡: {pretrained_path}')
                print_report(f'  ç­–ç•¥: åŠ è½½æ‰€æœ‰å…¼å®¹çš„æƒé‡ï¼ˆå…±äº«å±‚ + è¾“å‡ºå±‚ï¼Œæ¨¡ä»¿GHGNNæˆåŠŸç­–ç•¥ï¼‰')
                print_report(f'  æˆåŠŸåŠ è½½: {len(matched_keys)} å±‚')
                print_report(f'    - å…±äº«ç‰¹å¾æå–å±‚: {shared_layer_count} å±‚ï¼ˆgraphnet1/2, gnorm1/2, global_conv1ï¼‰')
                print_report(f'    - Task_Aè¾“å‡ºå±‚: {task_a_count} å±‚ï¼ˆmlp1a/mlp2a/mlp3aï¼‰')
                print_report(f'    - Task_Bè¾“å‡ºå±‚: {task_b_count} å±‚ï¼ˆmlp1b/mlp2b/mlp3bï¼‰')
                if other_count > 0:
                    print_report(f'    - å…¶ä»–å±‚: {other_count} å±‚')
                if attention_init_count > 0:
                    print_report(f'    - æ³¨æ„åŠ›æœºåˆ¶å±‚: {attention_init_count} å±‚ï¼ˆä½¿ç”¨Xavieråˆå§‹åŒ–ï¼Œå› ä¸ºæ— æ³•ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼‰')
                if len(skipped_keys) > 0:
                    print_report(f'  è·³è¿‡: {len(skipped_keys)} å±‚ï¼ˆä¸å…¼å®¹ï¼‰')
                    # è¾“å‡ºå‰10ä¸ªè·³è¿‡çš„é”®åï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜
                    if len(skipped_keys) <= 10:
                        for skip_info in skipped_keys:
                            print_report(f'    - {skip_info}')
                    else:
                        for skip_info in skipped_keys[:10]:
                            print_report(f'    - {skip_info}')
                        print_report(f'    ... è¿˜æœ‰ {len(skipped_keys) - 10} ä¸ªé”®è¢«è·³è¿‡')
                if len(matched_keys) == 0:
                    print_report('  è­¦å‘Š: æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æƒé‡ï¼Œå¯èƒ½æ¶æ„ä¸åŒ¹é…')
                    print_report('  å»ºè®®: å¦‚æœä¸éœ€è¦é¢„è®­ç»ƒæƒé‡ï¼Œè¯·åœ¨è¶…å‚æ•°ä¸­è®¾ç½® use_pretrained=False')
                else:
                    # è®¾ç½®åˆ†é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œä¿æŠ¤é¢„è®­ç»ƒæƒé‡
                    if fine_tune_stage == 'none' or fine_tune_stage is None:
                        # ä¸ä½¿ç”¨ä»»ä½•å¾®è°ƒç­–ç•¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å­¦ä¹ ç‡è¿›è¡Œå¸¸è§„è®­ç»ƒ
                        # print_report(f'\n{"="*60}')
                        # print_report(f'ã€å¸¸è§„è®­ç»ƒç­–ç•¥ã€‘ä¸ä½¿ç”¨ä»»ä½•å¾®è°ƒç­–ç•¥')
                        # print_report(f'{"="*60}')
                        # print_report(f'  - å·²åŠ è½½é¢„è®­ç»ƒæƒé‡')
                        # print_report(f'  - ä½¿ç”¨åŸå§‹å­¦ä¹ ç‡: {original_lr:.6f}')
                        # print_report(f'  - æ‰€æœ‰å±‚éƒ½å‚ä¸è®­ç»ƒï¼Œä¸é™ä½å­¦ä¹ ç‡ï¼Œä¸å†»ç»“å±‚')
                        # print_report(f'{"="*60}\n')
                        # ä¸ä¿®æ”¹lrï¼Œä½¿ç”¨åŸå§‹å­¦ä¹ ç‡
                        pass  # ä¸åšä»»ä½•æ“ä½œï¼Œä½¿ç”¨åŸå§‹å­¦ä¹ ç‡
                    elif fine_tune_stage == 'two_stage':
                        # ä¸¤é˜¶æ®µå¾®è°ƒï¼šå…ˆåªè®­ç»ƒè¾“å‡ºå±‚ï¼Œå†è®­ç»ƒæ‰€æœ‰å±‚
                        freeze_epochs = hyperparameters.get('freeze_epochs', max(1, int(n_epochs * 0.2)))  # é»˜è®¤20%çš„epochå†»ç»“å…±äº«å±‚
                        freeze_shared_layers = True
                        # print_report(f'\n{"="*60}')
                        # print_report(f'ã€ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ã€‘ä¿æŠ¤é¢„è®­ç»ƒæƒé‡')
                        # print_report(f'{"="*60}')
                        # print_report(f'  é˜¶æ®µ1: å‰ {freeze_epochs} è½®å†»ç»“å…±äº«å±‚ï¼Œåªè®­ç»ƒè¾“å‡ºå±‚')
                        # print_report(f'     - å­¦ä¹ ç‡: {original_lr:.6f}')
                        # print_report(f'     - ä¿æŠ¤é¢„è®­ç»ƒçš„ç‰¹å¾æå–èƒ½åŠ›')
                        fine_tune_lr_stage2 = original_lr * fine_tune_lr_factor
                        reduction_factor_str = f"{1/fine_tune_lr_factor:.1f}å€" if fine_tune_lr_factor < 1 else "ä¸å˜"
                        # print_report(f'  é˜¶æ®µ2: å {n_epochs - freeze_epochs} è½®è§£å†»æ‰€æœ‰å±‚ï¼Œå…¨é‡å¾®è°ƒ')
                        # print_report(f'     - å­¦ä¹ ç‡: {fine_tune_lr_stage2:.6f} (é™ä½{reduction_factor_str})')
                        # print_report(f'     - ç²¾ç»†è°ƒæ•´æ‰€æœ‰å‚æ•°')
                        # print_report(f'{"="*60}\n')
                    elif fine_tune_stage == 'output_only':
                        # åªè®­ç»ƒè¾“å‡ºå±‚ï¼Œå®Œå…¨å†»ç»“å…±äº«å±‚
                        freeze_shared_layers = True
                        freeze_epochs = n_epochs  # å§‹ç»ˆå†»ç»“
                        # print_report(f'\n{"="*60}')
                        # print_report(f'ã€è¾“å‡ºå±‚å¾®è°ƒç­–ç•¥ã€‘å®Œå…¨ä¿æŠ¤é¢„è®­ç»ƒæƒé‡')
                        # print_report(f'{"="*60}')
                        # print_report(f'  - å†»ç»“æ‰€æœ‰å…±äº«å±‚ï¼ˆgraphnet1/2, gnorm1/2, global_conv1ï¼‰')
                        # print_report(f'  - åªè®­ç»ƒè¾“å‡ºå±‚ï¼ˆmlp1a/mlp2a/mlp3a, mlp1b/mlp2b/mlp3bï¼‰')
                        # print_report(f'  - å­¦ä¹ ç‡: {original_lr:.6f}')
                        # print_report(f'{"="*60}\n')
                    else:
                        # å…¨é‡å¾®è°ƒï¼Œä½†ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
                        fine_tune_lr = original_lr * fine_tune_lr_factor
                        reduction_factor_str = f"{1/fine_tune_lr_factor:.1f}å€" if fine_tune_lr_factor < 1 else "ä¸å˜"
                        # print_report(f'\n{"="*60}')
                        # print_report(f'ã€å…¨é‡å¾®è°ƒç­–ç•¥ã€‘ä½¿ç”¨å°å­¦ä¹ ç‡ä¿æŠ¤é¢„è®­ç»ƒæƒé‡')
                        # print_report(f'{"="*60}')
                        # print_report(f'  - å­¦ä¹ ç‡ä» {original_lr:.6f} é™ä½è‡³ {fine_tune_lr:.6f} (é™ä½{reduction_factor_str})')
                        # print_report(f'  - æ‰€æœ‰å±‚éƒ½å‚ä¸è®­ç»ƒï¼Œä½†ä½¿ç”¨å°å­¦ä¹ ç‡é¿å…ç ´åé¢„è®­ç»ƒæƒé‡')
                        # print_report(f'{"="*60}\n')
                        lr = fine_tune_lr
                    
                    # å¦‚æœä½¿ç”¨å†»ç»“ç­–ç•¥ï¼Œç°åœ¨è®¾ç½®å†»ç»“çŠ¶æ€
                    if freeze_shared_layers:
                        # å†»ç»“å…±äº«ç‰¹å¾æå–å±‚ï¼ˆgraphnet1, graphnet2, gnorm1, gnorm2, global_conv1ï¼‰
                        frozen_count = 0
                        trainable_count = 0
                        for name, param in model.named_parameters():
                            if any(layer in name for layer in ['graphnet1', 'graphnet2', 'gnorm1', 'gnorm2', 'global_conv1']):
                                param.requires_grad = False
                                frozen_count += 1
                            else:
                                trainable_count += 1
                        
                        frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)
                        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                        print_report(f'å†»ç»“å‚æ•°å±‚æ•°: {frozen_count}, å¯è®­ç»ƒå±‚æ•°: {trainable_count}')
                        print_report(f'å†»ç»“å‚æ•°æ•°é‡: {frozen_params:,}, å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}')
                        print_report(f'å†»ç»“æ¯”ä¾‹: {frozen_params/(frozen_params+trainable_params)*100:.1f}%')
            except Exception as e:
                print_report(f'è­¦å‘Š: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡å¤±è´¥: {e}')
                print_report('ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡')
        else:
            if not use_pretrained:
                print_report('é¢„è®­ç»ƒæƒé‡å·²ç¦ç”¨ï¼ˆuse_pretrained=Falseï¼‰ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–')
            else:
                print_report(f'è­¦å‘Š: é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}')
                print_report('ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡')
    
    # å¦‚æœæœ‰æ£€æŸ¥ç‚¹ï¼ŒåŠ è½½æ£€æŸ¥ç‚¹ï¼ˆè¦†ç›–é¢„è®­ç»ƒæƒé‡ï¼‰
    if has_checkpoint:
        resumed_from_checkpoint = True  # æ ‡è®°ä»æ£€æŸ¥ç‚¹æ¢å¤
        print_report(f'å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_to_load}')
        try:
            checkpoint = torch.load(checkpoint_to_load, map_location=torch.device(available_device), weights_only=False)
            
            # ğŸ”‘ æ£€æŸ¥ï¼šå¦‚æœæä¾›äº†éªŒè¯é›†ä½†checkpointä¸­æ²¡æœ‰éªŒè¯é›†å†å²ï¼Œåˆ™ä¸ä½¿ç”¨checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ
            if val_loader is not None and 'mae_valid' not in checkpoint:
                print_report('')
                print_report('='*60)
                print_report('âš ï¸  æ£€æµ‹åˆ°éªŒè¯é›†ï¼Œä½†checkpointä¸­æ— éªŒè¯é›†å†å²æ•°æ®')
                print_report('   ä¸ºç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼Œå°†æ”¾å¼ƒcheckpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ')
                print_report('   åŸå› ï¼šæ—§checkpointä¸åŒ…å«éªŒè¯é›†å†å²ï¼Œæ— æ³•å‡†ç¡®æ¢å¤å†å²æœ€ä½³MAEå’ŒRÂ²')
                print_report('   æç¤ºï¼šæ–°è®­ç»ƒäº§ç”Ÿçš„checkpointå°†åŒ…å«å®Œæ•´çš„éªŒè¯é›†å†å²')
                print_report('='*60)
                print_report('')
                # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©exceptå—å¤„ç†ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ
                raise ValueError('Checkpointç¼ºå°‘éªŒè¯é›†å†å²ï¼Œæ— æ³•å‡†ç¡®æ¢å¤')
            
            # éªŒè¯è¶…å‚æ•°ä¸€è‡´æ€§
            if 'hyperparameters' in checkpoint:
                saved_hparams = checkpoint['hyperparameters']
                if saved_hparams.get('hidden_dim') != hyperparameters.get('hidden_dim'):
                    print_report(f'è­¦å‘Š: hidden_dimä¸åŒ¹é… (æ£€æŸ¥ç‚¹: {saved_hparams.get("hidden_dim")}, å½“å‰: {hyperparameters.get("hidden_dim")})')
                if saved_hparams.get('batch_size') != hyperparameters.get('batch_size'):
                    print_report(f'è­¦å‘Š: batch_sizeä¸åŒ¹é… (æ£€æŸ¥ç‚¹: {saved_hparams.get("batch_size")}, å½“å‰: {hyperparameters.get("batch_size")})')
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            # ä½¿ç”¨ strict=False ä»¥å¤„ç†æ¶æ„å˜åŒ–ï¼ˆä¾‹å¦‚ input_projection å±‚çš„åŠ¨æ€åˆ›å»º/åˆ é™¤ï¼‰
            # å½“ attention_weight ä¸åŒæ—¶ï¼Œæ¨¡å‹å¯èƒ½åŒ…å«æˆ–ä¸åŒ…å« input_projection å±‚
            
            # å¤„ç†.pthæ–‡ä»¶ï¼šå¯èƒ½æ˜¯ç›´æ¥çš„state_dictï¼Œä¹Ÿå¯èƒ½æ˜¯åŒ…å«model_state_dictçš„å­—å…¸
            if is_pth_file:
                # .pthæ–‡ä»¶å¯èƒ½æ˜¯ç›´æ¥çš„state_dict
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    checkpoint_state = checkpoint['model_state_dict']
                elif isinstance(checkpoint, dict) and not any(k in checkpoint for k in ['optimizer_state_dict', 'scheduler_state_dict', 'epoch', 'best_MAE']):
                    # å¦‚æœæ˜¯ä¸€ä¸ªå­—å…¸ä½†ä¸åŒ…å«è®­ç»ƒçŠ¶æ€ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯ç›´æ¥çš„state_dict
                    checkpoint_state = checkpoint
                else:
                    # å¦åˆ™å‡è®¾æ˜¯ç›´æ¥çš„state_dict
                    checkpoint_state = checkpoint
            else:
                # æ£€æŸ¥ç‚¹æ–‡ä»¶åº”è¯¥åŒ…å«model_state_dict
                if 'model_state_dict' in checkpoint:
                    checkpoint_state = checkpoint['model_state_dict']
                else:
                    print_report('è­¦å‘Š: æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­æœªæ‰¾åˆ°model_state_dictï¼Œå°è¯•ç›´æ¥ä½¿ç”¨æ•´ä¸ªcheckpointä½œä¸ºstate_dict')
                    checkpoint_state = checkpoint
            
            # æ£€æŸ¥æ£€æŸ¥ç‚¹ä¸­æ˜¯å¦æœ‰ input_projection å±‚
            has_input_proj_in_ckpt = any('input_projection' in k for k in checkpoint_state.keys())
            current_attention_weight = hyperparameters.get('attention_weight', 1.0)
            
            # æ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦å·²ç»åŒ…å« input_projection å±‚
            model_state_before = model.state_dict()
            has_input_proj_in_model = any('input_projection' in k for k in model_state_before.keys())
            
            # å¦‚æœæ£€æŸ¥ç‚¹ä¸­æœ‰ input_projection ä½†å½“å‰æ¨¡å‹æ²¡æœ‰ï¼Œéœ€è¦å…ˆè§¦å‘åˆ›å»º
            # æ— è®º attention_weight æ˜¯å¤šå°‘ï¼Œåªè¦æ£€æŸ¥ç‚¹ä¸­æœ‰ä½†æ¨¡å‹ä¸­æ²¡æœ‰ï¼Œå°±åˆ›å»º
            if has_input_proj_in_ckpt and not has_input_proj_in_model:
                print_report('')
                print_report('='*60)
                print_report('ã€åŠ¨æ€åˆ›å»º input_projection å±‚ã€‘')
                print_report('='*60)
                print_report(f'æ£€æŸ¥ç‚¹ä¸­åŒ…å« input_projection å±‚ï¼Œä½†å½“å‰æ¨¡å‹ä¸­æ²¡æœ‰è¿™äº›å±‚')
                print_report(f'å½“å‰ attention_weight={current_attention_weight}')
                print_report('å°†ä¸´æ—¶è®¾ç½® attention_weight < 1.0 æ¥è§¦å‘ input_projection å±‚çš„åˆ›å»ºï¼Œç„¶ååŠ è½½æƒé‡')
                print_report('='*60)
                print_report('')
                
                try:
                    # ä¸´æ—¶ä¿®æ”¹æ¨¡å‹ä¸­æ‰€æœ‰ NodeModel çš„ attention_weight ä»¥è§¦å‘ input_projection åˆ›å»º
                    def set_attention_weight_recursive(module, value):
                        if hasattr(module, 'attention_weight'):
                            module.attention_weight = value
                        for child in module.children():
                            set_attention_weight_recursive(child, value)
                    
                    # ä¿å­˜åŸå§‹ attention_weight
                    original_attention_weight = current_attention_weight
                    
                    # ä¸´æ—¶è®¾ç½® attention_weight = 0.5 ä»¥è§¦å‘ input_projection åˆ›å»º
                    set_attention_weight_recursive(model, 0.5)
                    
                    # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€ä¸ªæ ·æœ¬æ¥è§¦å‘ forwardï¼ˆåˆ›å»º input_projection å±‚ï¼‰
                    # æ³¨æ„ï¼šæ­¤æ—¶ df, graphs_solv, graphs_solu åº”è¯¥å·²ç»å‡†å¤‡å¥½äº†
                    # get_dataloader_pairs_T å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œæ— éœ€é‡å¤å¯¼å…¥
                    temp_loader = get_dataloader_pairs_T(
                        df, 
                        df.index.tolist()[:1],  # åªç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬
                        graphs_solv,
                        graphs_solu,
                        batch_size=1, 
                        shuffle=False, 
                        drop_last=False,
                        num_workers=0,  # ä½¿ç”¨å•è¿›ç¨‹é¿å…é—®é¢˜
                        pin_memory=False
                    )
                    
                    model.eval()
                    with torch.no_grad():
                        for batch_data in temp_loader:
                            if len(batch_data) == 3:
                                batch_solvent, batch_solute, T = batch_data
                                batch_solvent = batch_solvent.to(device)
                                batch_solute = batch_solute.to(device)
                                T = T.to(device)
                                # è¿è¡Œä¸€æ¬¡ forward æ¥è§¦å‘ input_projection çš„åˆ›å»º
                                _ = model(batch_solvent, batch_solute, T)
                                print_report('âœ“ input_projection å±‚å·²åˆ›å»º')
                                break
                    
                    # æ£€æŸ¥å±‚æ˜¯å¦çœŸçš„åˆ›å»ºäº†ï¼Œå¹¶ç«‹å³å°è¯•åŠ è½½æƒé‡
                    model_state_temp = model.state_dict()
                    input_proj_created = [k for k in checkpoint_state.keys() if 'input_projection' in k and k in model_state_temp]
                    if input_proj_created:
                        print_report(f'âœ“ æ£€æµ‹åˆ° {len(input_proj_created)} ä¸ª input_projection å±‚å·²åˆ›å»ºï¼Œç«‹å³åŠ è½½æƒé‡...')
                        for key in input_proj_created:
                            if model_state_temp[key].shape == checkpoint_state[key].shape:
                                model_state_temp[key] = checkpoint_state[key]
                        model.load_state_dict(model_state_temp, strict=False)
                        print_report(f'âœ“ å·²åŠ è½½ {len(input_proj_created)} ä¸ª input_projection å±‚çš„æƒé‡')
                    
                    # æ¢å¤åŸå§‹çš„ attention_weight
                    set_attention_weight_recursive(model, original_attention_weight)
                    print_report(f'âœ“ å·²æ¢å¤ attention_weight={original_attention_weight}')
                    
                except Exception as e:
                    print_report(f'âš ï¸ è­¦å‘Š: åˆ›å»º input_projection å±‚æ—¶å‡ºé”™: {e}')
                    print_report('   å°†å°è¯•ç›´æ¥åŠ è½½æƒé‡ï¼ˆå¯èƒ½ä¼šè·³è¿‡ input_projection å±‚ï¼‰')
                    import traceback
                    traceback.print_exc()
            
            model_state = model.state_dict()
            
            # è¿‡æ»¤æ‰æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„é”®ï¼Œå¹¶è¯¦ç»†è®°å½•åŠ è½½æƒ…å†µ
            filtered_state = {}
            skipped_keys = []
            input_proj_keys = []  # å•ç‹¬æ”¶é›† input_projection é”®
            matched_keys = []  # è®°å½•æˆåŠŸåŠ è½½çš„é”®
            shape_mismatch_keys = []  # è®°å½•å½¢çŠ¶ä¸åŒ¹é…çš„é”®
            
            # ç»Ÿè®¡åŠ è½½çš„å±‚ç±»å‹
            layer_stats = {
                'graphnet': 0,
                'gnorm': 0,
                'global_conv': 0,
                'mlp': 0,
                'input_projection': 0,
                'ext_attention': 0,
                'other': 0
            }
            
            for key, value in checkpoint_state.items():
                if key in model_state:
                    if model_state[key].shape == value.shape:
                        filtered_state[key] = value
                        matched_keys.append(key)
                        # ç»Ÿè®¡å±‚ç±»å‹
                        if 'input_projection' in key:
                            layer_stats['input_projection'] += 1
                        elif 'graphnet' in key:
                            layer_stats['graphnet'] += 1
                        elif 'gnorm' in key:
                            layer_stats['gnorm'] += 1
                        elif 'global_conv' in key:
                            layer_stats['global_conv'] += 1
                        elif 'mlp' in key:
                            layer_stats['mlp'] += 1
                        elif 'ext_attention' in key:
                            layer_stats['ext_attention'] += 1
                        else:
                            layer_stats['other'] += 1
                    else:
                        shape_mismatch_keys.append(key)
                        skipped_keys.append(f"Shape mismatch: {key} (checkpoint: {value.shape}, model: {model_state[key].shape})")
                else:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ input_projection é”®
                    if 'input_projection' in key:
                        input_proj_keys.append(key)
                    skipped_keys.append(f"Key not in model: {key}")
            
            # åŠ è½½æƒé‡ï¼ˆå·²åœ¨ä¸Šé¢å®Œæˆï¼‰
            # model.load_state_dict(filtered_state, strict=False)  # å·²åœ¨ä¸Šé¢è°ƒç”¨
            
            # è¯¦ç»†æŠ¥å‘ŠåŠ è½½æƒ…å†µ
            print_report('')
            print_report('='*60)
            print_report('ã€æ£€æŸ¥ç‚¹æƒé‡åŠ è½½æŠ¥å‘Šã€‘')
            print_report('='*60)
            print_report(f'æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_to_load}')
            print_report(f'æ£€æŸ¥ç‚¹ä¸­çš„æ€»é”®æ•°: {len(checkpoint_state)}')
            print_report(f'æˆåŠŸåŠ è½½çš„å±‚æ•°: {len(matched_keys)}')
            print_report('')
            print_report('æŒ‰å±‚ç±»å‹ç»Ÿè®¡:')
            if layer_stats['graphnet'] > 0:
                print_report(f'  âœ“ GraphNetå±‚: {layer_stats["graphnet"]} å±‚')
            if layer_stats['gnorm'] > 0:
                print_report(f'  âœ“ GraphNormå±‚: {layer_stats["gnorm"]} å±‚')
            if layer_stats['global_conv'] > 0:
                print_report(f'  âœ“ GlobalConvå±‚: {layer_stats["global_conv"]} å±‚')
            if layer_stats['mlp'] > 0:
                print_report(f'  âœ“ MLPè¾“å‡ºå±‚: {layer_stats["mlp"]} å±‚')
            if layer_stats['input_projection'] > 0:
                print_report(f'  âœ“ InputProjectionå±‚: {layer_stats["input_projection"]} å±‚')
            if layer_stats['ext_attention'] > 0:
                print_report(f'  âœ“ ExternalAttentionå±‚: {layer_stats["ext_attention"]} å±‚')
            if layer_stats['other'] > 0:
                print_report(f'  âœ“ å…¶ä»–å±‚: {layer_stats["other"]} å±‚')
            print_report('')
            
            # å¦‚æœæ£€æŸ¥ç‚¹ä¸­æœ‰ input_projection ä½†å½“å‰æ¨¡å‹ä»ç„¶æ²¡æœ‰ï¼Œå°è¯•å†æ¬¡åˆ›å»º
            if input_proj_keys:
                print_report('')
                print_report('='*60)
                print_report(f'âš ï¸ æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ä¸­åŒ…å« {len(input_proj_keys)} ä¸ª input_projection å±‚é”®ï¼Œä½†å½“å‰æ¨¡å‹ä¸­æ²¡æœ‰')
                print_report('='*60)
                print_report(f'   å½“å‰ attention_weight é…ç½®: {current_attention_weight}')
                print_report(f'   å°è¯•å¼ºåˆ¶åˆ›å»º input_projection å±‚ä»¥åŠ è½½è¿™äº›æƒé‡...')
                print_report('='*60)
                print_report('')
                
                try:
                    # å†æ¬¡å°è¯•åˆ›å»º input_projection å±‚
                    def set_attention_weight_recursive(module, value):
                        if hasattr(module, 'attention_weight'):
                            module.attention_weight = value
                        for child in module.children():
                            set_attention_weight_recursive(child, value)
                    
                    # ä¸´æ—¶è®¾ç½® attention_weight = 0.5 ä»¥è§¦å‘ input_projection åˆ›å»º
                    set_attention_weight_recursive(model, 0.5)
                    
                    # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€ä¸ªæ ·æœ¬æ¥è§¦å‘ forwardï¼ˆåˆ›å»º input_projection å±‚ï¼‰
                    temp_loader = get_dataloader_pairs_T(
                        df, 
                        df.index.tolist()[:1],  # åªç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬
                        graphs_solv,
                        graphs_solu,
                        batch_size=1, 
                        shuffle=False, 
                        drop_last=False,
                        num_workers=0,
                        pin_memory=False
                    )
                    
                    model.eval()
                    with torch.no_grad():
                        for batch_data in temp_loader:
                            if len(batch_data) == 3:
                                batch_solvent, batch_solute, T = batch_data
                                batch_solvent = batch_solvent.to(device)
                                batch_solute = batch_solute.to(device)
                                T = T.to(device)
                                # è¿è¡Œä¸€æ¬¡ forward æ¥è§¦å‘ input_projection çš„åˆ›å»º
                                _ = model(batch_solvent, batch_solute, T)
                                break
                    
                    # æ¢å¤åŸå§‹çš„ attention_weight
                    set_attention_weight_recursive(model, current_attention_weight)
                    
                    # é‡æ–°æ£€æŸ¥æ¨¡å‹çŠ¶æ€ï¼Œå°è¯•åŠ è½½ input_projection æƒé‡
                    model_state_after = model.state_dict()
                    input_proj_loaded = 0
                    for key in input_proj_keys:
                        if key in model_state_after:
                            if model_state_after[key].shape == checkpoint_state[key].shape:
                                model_state_after[key] = checkpoint_state[key]
                                input_proj_loaded += 1
                    
                    if input_proj_loaded > 0:
                        model.load_state_dict(model_state_after, strict=False)
                        print_report(f'âœ“ æˆåŠŸåˆ›å»ºå¹¶åŠ è½½äº† {input_proj_loaded} ä¸ª input_projection å±‚')
                        print_report(f'âœ“ å·²æ¢å¤ attention_weight={current_attention_weight}')
                        # æ›´æ–° filtered_state å’Œ matched_keysï¼Œç§»é™¤å·²åŠ è½½çš„ input_proj_keys
                        for key in input_proj_keys:
                            if key in model_state_after:
                                filtered_state[key] = checkpoint_state[key]
                                matched_keys.append(key)
                                if key in input_proj_keys:
                                    input_proj_keys.remove(key)
                                layer_stats['input_projection'] += 1
                    else:
                        print_report(f'âš ï¸ æ— æ³•åˆ›å»ºæˆ–åŠ è½½ input_projection å±‚')
                        print_report(f'   æ£€æŸ¥ç‚¹ä¸­çš„ input_projection å±‚é”®:')
                        if len(input_proj_keys) <= 10:
                            for key in input_proj_keys:
                                print_report(f'     - {key}')
                        else:
                            for key in input_proj_keys[:10]:
                                print_report(f'     - {key}')
                            print_report(f'     ... è¿˜æœ‰ {len(input_proj_keys) - 10} ä¸ªé”®æœªæ˜¾ç¤º')
                    
                except Exception as e:
                    print_report(f'âš ï¸ å¼ºåˆ¶åˆ›å»º input_projection å±‚æ—¶å‡ºé”™: {e}')
                    print_report(f'   æ£€æŸ¥ç‚¹ä¸­çš„ input_projection å±‚é”®:')
                    if len(input_proj_keys) <= 10:
                        for key in input_proj_keys:
                            print_report(f'     - {key}')
                    else:
                        for key in input_proj_keys[:10]:
                            print_report(f'     - {key}')
                        print_report(f'     ... è¿˜æœ‰ {len(input_proj_keys) - 10} ä¸ªé”®æœªæ˜¾ç¤º')
                    import traceback
                    traceback.print_exc()
                
                print_report('='*60)
                print_report('')
            
            # æŠ¥å‘Šå½¢çŠ¶ä¸åŒ¹é…çš„é”®
            if shape_mismatch_keys:
                print_report(f'âš ï¸ å½¢çŠ¶ä¸åŒ¹é…çš„å±‚æ•°: {len(shape_mismatch_keys)}')
                if len(shape_mismatch_keys) <= 5:
                    for key in shape_mismatch_keys:
                        print_report(f'    - {key}')
                else:
                    for key in shape_mismatch_keys[:5]:
                        print_report(f'    - {key}')
                    print_report(f'    ... è¿˜æœ‰ {len(shape_mismatch_keys) - 5} ä¸ªé”®æœªæ˜¾ç¤º')
                print_report('')
            
                print_report('='*60)
                print_report('')
            
            # æŠ¥å‘Šå½¢çŠ¶ä¸åŒ¹é…çš„é”®
            if shape_mismatch_keys:
                print_report(f'âš ï¸ å½¢çŠ¶ä¸åŒ¹é…çš„å±‚æ•°: {len(shape_mismatch_keys)}')
                if len(shape_mismatch_keys) <= 5:
                    for key in shape_mismatch_keys:
                        print_report(f'    - {key}')
                else:
                    for key in shape_mismatch_keys[:5]:
                        print_report(f'    - {key}')
                    print_report(f'    ... è¿˜æœ‰ {len(shape_mismatch_keys) - 5} ä¸ªé”®æœªæ˜¾ç¤º')
                print_report('')
            
            print_report('='*60)
            print_report('')
            
            if skipped_keys:
                # ç»Ÿè®¡è·³è¿‡çš„é”®çš„ç±»å‹ï¼ˆæ’é™¤ input_projectionï¼Œå› ä¸ºå·²ç»å•ç‹¬å¤„ç†ï¼‰
                other_keys = [k for k in skipped_keys if 'input_projection' not in k]
                
                if other_keys:
                    print_report(f'  è·³è¿‡ {len(other_keys)} ä¸ªå…¶ä»–ä¸å…¼å®¹çš„é”®ï¼ˆå¯èƒ½æ˜¯ç”±äºæ¶æ„å˜åŒ–ï¼‰')
                    if len(other_keys) <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡é•¿
                        for key in other_keys[:5]:
                            print_report(f'    - {key}')
                    if len(other_keys) > 5:
                        print_report(f'    ... è¿˜æœ‰ {len(other_keys) - 5} ä¸ªé”®æœªæ˜¾ç¤º')
                
                # ä¿®å¤ï¼šæ ¹æ®å½“å‰çš„ fine_tune_stage é‡æ–°è®¾ç½®å‚æ•°çš„ requires_grad
                # é¿å…ä»æ£€æŸ¥ç‚¹åŠ è½½æ—¶æ¢å¤é”™è¯¯çš„å†»ç»“çŠ¶æ€
                if fine_tune_stage == 'none' or fine_tune_stage is None:
                    # ä¸å†»ç»“ä»»ä½•å±‚ï¼Œæ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒ
                    for param in model.parameters():
                        param.requires_grad = True
                    print_report('å·²é‡ç½®æ‰€æœ‰å‚æ•°ä¸ºå¯è®­ç»ƒçŠ¶æ€ï¼ˆfine_tune_stage=noneï¼‰')
                elif fine_tune_stage == 'two_stage':
                    # å…ˆè§£å†»æ‰€æœ‰å‚æ•°ï¼Œå†»ç»“é€»è¾‘ä¼šåœ¨è®­ç»ƒå¾ªç¯ä¸­æ ¹æ® freeze_epochs å¤„ç†
                    for param in model.parameters():
                        param.requires_grad = True
                    print_report('å·²é‡ç½®æ‰€æœ‰å‚æ•°ä¸ºå¯è®­ç»ƒçŠ¶æ€ï¼ˆfine_tune_stage=two_stageï¼Œå°†åœ¨è®­ç»ƒå¾ªç¯ä¸­å¤„ç†å†»ç»“ï¼‰')
                elif fine_tune_stage == 'output_only':
                    # å†»ç»“å…±äº«å±‚
                    frozen_count = 0
                    for name, param in model.named_parameters():
                        if any(layer in name for layer in ['graphnet1', 'graphnet2', 'gnorm1', 'gnorm2', 'global_conv1']):
                            param.requires_grad = False
                            frozen_count += 1
                        else:
                            param.requires_grad = True
                    print_report(f'å·²æ ¹æ® fine_tune_stage=output_only è®¾ç½®å†»ç»“çŠ¶æ€ï¼ˆå†»ç»“ {frozen_count} å±‚ï¼‰')
            
            # å¦‚æœæ˜¯.pthæ–‡ä»¶ï¼Œä¸åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€å’Œè®­ç»ƒå†å²ï¼Œè·³è¿‡è¿™äº›åŠ è½½
            if is_pth_file:
                print_report('âš ï¸ ä».pthæ–‡ä»¶åŠ è½½ï¼šä¸åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€å’Œè®­ç»ƒå†å²ï¼Œè®­ç»ƒå°†ä»ç¬¬0è½®å¼€å§‹')
                optimizer_state_dict = None
                scheduler_state_dict = None
                start_epoch = 0
                # ä¸æ¢å¤è®­ç»ƒå†å²ï¼Œä½¿ç”¨åˆå§‹å€¼
            else:
                # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå°†åœ¨optimizeråˆ›å»ºååŠ è½½ï¼‰
                optimizer_state_dict = checkpoint.get('optimizer_state_dict', None)
                if optimizer_state_dict is not None:
                    print_report('æ£€æŸ¥ç‚¹ä¸­åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå°†åœ¨optimizeråˆ›å»ºååŠ è½½')
                    # æå‰æ£€æŸ¥ä¼˜åŒ–å™¨é…ç½®ï¼Œä»¥ä¾¿åœ¨åˆ›å»ºä¼˜åŒ–å™¨æ—¶åŒ¹é…
                    checkpoint_param_groups = len(optimizer_state_dict.get('param_groups', []))
                    print_report(f'æ£€æŸ¥ç‚¹ä¸­çš„ä¼˜åŒ–å™¨å‚æ•°ç»„æ•°é‡: {checkpoint_param_groups}')
                    # å¦‚æœæ£€æŸ¥ç‚¹ä½¿ç”¨äº†åˆ†å±‚å­¦ä¹ ç‡ï¼Œæ ‡è®°ä»¥ä¾¿åç»­ä½¿ç”¨
                    if checkpoint_param_groups > 1:
                        checkpoint_uses_layered_lr = True
                        print_report(f'âœ“ å·²æ ‡è®°ï¼šæ£€æŸ¥ç‚¹ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡ï¼ˆ{checkpoint_param_groups}ä¸ªå‚æ•°ç»„ï¼‰ï¼Œå°†åœ¨åˆ›å»ºä¼˜åŒ–å™¨æ—¶åŒ¹é…æ­¤é…ç½®')
            
            # ä¿å­˜è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå°†åœ¨scheduleråˆ›å»ºååŠ è½½ï¼‰
            scheduler_state_dict = checkpoint.get('scheduler_state_dict', None)
            if scheduler_state_dict is not None:
                print_report('æ£€æŸ¥ç‚¹ä¸­åŒ…å«è°ƒåº¦å™¨çŠ¶æ€ï¼Œå°†åœ¨scheduleråˆ›å»ºååŠ è½½')
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            if 'best_model_state_dict' in checkpoint:
                best_model = checkpoint['best_model_state_dict']
                print_report('å·²åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€')
                # é‡è¦ï¼šå¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œä¼˜å…ˆä½¿ç”¨æœ€ä½³æ¨¡å‹çŠ¶æ€è€Œä¸æ˜¯æœ€åä¸€è½®çš„çŠ¶æ€
                # è¿™æ ·å¯ä»¥é¿å…ä»è¾ƒå·®çš„æ¨¡å‹çŠ¶æ€ç»§ç»­è®­ç»ƒ
                # ä½†éœ€è¦ç”¨æˆ·æ˜ç¡®æŒ‡å®šæ˜¯å¦ä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼ˆé€šè¿‡hyperparametersä¸­çš„use_best_model_on_resumeï¼‰
                use_best_model_on_resume = hyperparameters.get('use_best_model_on_resume', False)
                if use_best_model_on_resume:
                    print_report('âš ï¸ ä½¿ç”¨æœ€ä½³æ¨¡å‹çŠ¶æ€æ¢å¤è®­ç»ƒï¼ˆè€Œä¸æ˜¯æœ€åä¸€è½®çŠ¶æ€ï¼‰')
                    # ä½¿ç”¨æœ€ä½³æ¨¡å‹çŠ¶æ€è¦†ç›–å½“å‰æ¨¡å‹çŠ¶æ€
                    model.load_state_dict(best_model, strict=False)
                    print_report('å·²ç”¨æœ€ä½³æ¨¡å‹çŠ¶æ€æ›´æ–°å½“å‰æ¨¡å‹')
            
            # æ¢å¤è®­ç»ƒå†å²
            if 'epoch' in checkpoint:
                checkpoint_epoch = checkpoint['epoch']
                # å¦‚æœæŒ‡å®šäº†start_epoch_overrideï¼Œä½¿ç”¨æŒ‡å®šçš„å€¼ï¼›å¦åˆ™ä½¿ç”¨checkpointä¸­çš„epoch+1
                if start_epoch_override is not None:
                    start_epoch = start_epoch_override
                    print_report(f'æ£€æŸ¥ç‚¹ä¸­çš„epoch: {checkpoint_epoch}ï¼Œä½†å°†å¼ºåˆ¶ä»ç¬¬ {start_epoch} è½®å¼€å§‹è®­ç»ƒ')
                else:
                    start_epoch = checkpoint_epoch + 1
                    print_report(f'ä»ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒï¼ˆæ£€æŸ¥ç‚¹ä¸­çš„epoch: {checkpoint_epoch}ï¼‰')
            
            # æ¢å¤è®­ç»ƒå†å²
            if 'mae_train' in checkpoint:
                mae_train = checkpoint['mae_train']
                print_report(f'å·²æ¢å¤è®­ç»ƒMAEå†å² ({len(mae_train)} è½®)')
            
            if 'r2_train' in checkpoint:
                r2_train = checkpoint['r2_train']
                print_report(f'å·²æ¢å¤è®­ç»ƒRÂ²å†å² ({len(r2_train)} è½®)')
            
            # ğŸ”‘ æ¢å¤éªŒè¯é›†å†å²
            if 'mae_valid' in checkpoint:
                mae_valid = checkpoint['mae_valid']
                print_report(f'å·²æ¢å¤éªŒè¯é›†MAEå†å² ({len(mae_valid)} è½®)')
            
            if 'r2_valid' in checkpoint:
                r2_valid = checkpoint['r2_valid']
                print_report(f'å·²æ¢å¤éªŒè¯é›†RÂ²å†å² ({len(r2_valid)} è½®)')
            
            # æ¢å¤best_MAE
            if 'best_MAE' in checkpoint:
                # å¦‚æœæä¾›äº†éªŒè¯é›†ä¸”checkpointä¸­æœ‰éªŒè¯é›†å†å²ï¼Œä½¿ç”¨éªŒè¯é›†æœ€ä½³å€¼
                if val_loader is not None and len(mae_valid) > 0:
                    best_MAE = min(mae_valid)
                    print_report(f'ä»éªŒè¯é›†å†å²æ¢å¤æœ€ä½³MAE: {best_MAE:.6f}')
                # å¦‚æœæä¾›äº†éªŒè¯é›†ä½†checkpointä¸­æ²¡æœ‰éªŒè¯é›†å†å²ï¼Œé‡æ–°è®¡ç®—
                elif val_loader is not None:
                    print_report(f'ä»checkpointæ¢å¤ï¼šæ£€æµ‹åˆ°éªŒè¯é›†ï¼Œä½†æ— éªŒè¯é›†å†å²ï¼Œå°†é‡æ–°å¯»æ‰¾éªŒè¯é›†æœ€ä½³MAE')
                    best_MAE = np.inf
                # å¦‚æœæ²¡æœ‰æä¾›éªŒè¯é›†ï¼Œä½¿ç”¨checkpointä¸­çš„å€¼
                else:
                    best_MAE = checkpoint['best_MAE']
                    print_report(f'å½“å‰æœ€ä½³MAE: {best_MAE:.6f}')
            
            # è®°å½•æ¢å¤æ—¶çš„æœ€ä½³MAEï¼ˆç”¨äºåç»­åˆ¤æ–­æ˜¯å¦æ›´æ–°ï¼‰
            resumed_best_mae = best_MAE
            
            # æ¢å¤æ—©åœå™¨çŠ¶æ€
            if 'early_stopping_state' in checkpoint:
                es_state = checkpoint['early_stopping_state']
                early_stopper.best = es_state.get('best')
                early_stopper.num_bad = es_state.get('num_bad', 0)
                early_stopper.should_stop = es_state.get('should_stop', False)
                
                # å¦‚æœpatienceè¢«è®¾ç½®ä¸ºéå¸¸å¤§çš„å€¼ï¼ˆè¡¨ç¤ºç¦ç”¨æ—©åœï¼‰ï¼Œé‡ç½®should_stop
                if early_stop_resume_patience >= 99999:  # é˜ˆå€¼ï¼š99999æˆ–æ›´å¤§è¡¨ç¤ºç¦ç”¨æ—©åœ
                    if early_stopper.should_stop:
                        print_report(f'æ£€æµ‹åˆ°æ—©åœå·²ç¦ç”¨ï¼ˆpatience={early_stop_resume_patience}ï¼‰ï¼Œé‡ç½®æ—©åœå™¨çŠ¶æ€')
                        early_stopper.should_stop = False
                        early_stopper.num_bad = 0  # é‡ç½®è®¡æ•°ï¼Œå…è®¸ç»§ç»­è®­ç»ƒ
                
                print_report(f'å·²æ¢å¤æ—©åœå™¨çŠ¶æ€ (best: {early_stopper.best:.6f}, num_bad: {early_stopper.num_bad}, should_stop: {early_stopper.should_stop})')
            
            # ğŸ”‘ ä»æ£€æŸ¥ç‚¹æ¢å¤æ—¶ï¼Œå…ˆç¦ç”¨æ—©åœï¼Œå¾…éªŒè¯é›†æœ€ä½³MAEæ›´æ–°åå†æ¢å¤
            if resumed_from_checkpoint:
                early_stop_active = False
                early_stopper.should_stop = False
                early_stopper.num_bad = 0
                print_report(f'ğŸ”‘ ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼šå·²ç¦ç”¨æ—©åœï¼Œç­‰å¾…éªŒè¯é›†æœ€ä½³MAEæ›´æ–°åå†æ¢å¤')
                if resumed_best_mae is not None and resumed_best_mae != np.inf:
                    print_report(f'   å½“å‰æœ€ä½³MAE: {resumed_best_mae:.6f}ï¼Œå°†åœ¨æ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹åæ¢å¤æ—©åœ')
                else:
                    print_report(f'   å°†ç­‰å¾…éªŒè¯é›†æœ€ä½³MAEé¦–æ¬¡æ›´æ–°åæ¢å¤æ—©åœ')
            
            print_report('æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆï¼')
        except Exception as e:
            print_report(f'è­¦å‘Š: åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}')
            print_report('å°†ä»ç¬¬0è½®å¼€å§‹è®­ç»ƒ')
            start_epoch = 0
    else:
        if resume_checkpoint:
            print_report(f'è­¦å‘Š: æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {resume_checkpoint}')
            print_report('å°†ä»ç¬¬0è½®å¼€å§‹è®­ç»ƒ')
        else:
            print_report('æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°†ä»ç¬¬0è½®å¼€å§‹è®­ç»ƒ')

    # æ ¹æ® fine_tune_epochs è°ƒæ•´ç›®æ ‡è®­ç»ƒè½®æ•°ï¼ˆstart_epoch å·²ç¡®å®šï¼‰
    target_total_epochs = n_epochs
    if fine_tune_epochs is not None:
        # ç¡®ä¿è‡³å°‘è¿˜èƒ½è®­ç»ƒ1è½®
        target_total_epochs = max(start_epoch + fine_tune_epochs, start_epoch + 1)
        print_report(f'Fine-tuneæ¨¡å¼ï¼šå°†åœ¨å½“å‰èµ·ç‚¹ï¼ˆç¬¬ {start_epoch} è½®ï¼‰åŸºç¡€ä¸Šè¿½åŠ  {fine_tune_epochs} è½®ï¼Œæ€»è®­ç»ƒè½®æ¬¡è°ƒæ•´ä¸º {target_total_epochs}')
    total_epochs = target_total_epochs
    
    # åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡åï¼Œåˆ›å»ºä¼˜åŒ–å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆ›å»ºï¼‰
    # ä»è¶…å‚æ•°ä¸­è·å–weight_decayï¼Œé»˜è®¤1e-2
    weight_decay = hyperparameters.get('weight_decay', 1e-2)
    
    if 'optimizer' not in locals():
        # å¦‚æœä½¿ç”¨å†»ç»“ç­–ç•¥ï¼Œåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        if freeze_shared_layers:
            trainable_params = [p for p in model.parameters() if p.requires_grad]
            optimizer = torch.optim.AdamW(trainable_params, lr=original_lr, weight_decay=weight_decay)
            print_report(f'ä¼˜åŒ–å™¨: åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°')
        else:
            # å…¨é‡å¾®è°ƒï¼šä½¿ç”¨ä¸åŒå­¦ä¹ ç‡ï¼ˆå…±äº«å±‚ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼Œæ³¨æ„åŠ›å±‚ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼‰
            if use_pretrained and not has_checkpoint and fine_tune_stage == 'full':
                # ä¸ºå…±äº«å±‚ã€æ³¨æ„åŠ›å±‚å’Œè¾“å‡ºå±‚è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
                # ä¼˜åŒ–ç­–ç•¥ï¼šæ ¹æ®attention_weightåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
                attention_weight = hyperparameters.get('attention_weight', 1.0)
                
                # æ ¹æ®attention_weightåŠ¨æ€è°ƒæ•´å…±äº«å±‚å­¦ä¹ ç‡
                if attention_weight <= 0.1:
                    shared_lr_factor = max(fine_tune_lr_factor, 0.08)  # æ³¨æ„åŠ›å½±å“å°ï¼Œå¯ä»¥ç¨å¤§çš„å­¦ä¹ ç‡
                elif attention_weight >= 0.9:
                    shared_lr_factor = min(fine_tune_lr_factor, 0.02)  # æ³¨æ„åŠ›å½±å“å¤§ï¼Œéœ€è¦å¾ˆå°çš„å­¦ä¹ ç‡
                else:
                    # çº¿æ€§æ’å€¼ï¼š0.1->0.08, 0.9->0.02
                    base_factor = 0.08 - 0.075 * (attention_weight - 0.1) / 0.8
                    shared_lr_factor = min(fine_tune_lr_factor, base_factor)
                
                # æ ¹æ®attention_weightåŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›å±‚å­¦ä¹ ç‡
                if attention_weight <= 0.1:
                    attention_lr_factor = 2.5
                elif attention_weight >= 0.9:
                    attention_lr_factor = 1.5
                else:
                    attention_lr_factor = 2.5 - 1.25 * (attention_weight - 0.1) / 0.8
                
                shared_lr = original_lr * shared_lr_factor
                attention_lr = original_lr * attention_lr_factor
                shared_params = []
                attention_params = []
                output_params = []
                for name, param in model.named_parameters():
                    if any(layer in name for layer in ['graphnet1', 'graphnet2', 'gnorm1', 'gnorm2', 'global_conv1']):
                        shared_params.append(param)
                    elif 'ext_attention' in name and ('Mk' in name or 'Mv' in name):
                        attention_params.append(param)
                    else:
                        output_params.append(param)
                
                # æ„å»ºå‚æ•°ç»„
                param_groups = [
                    {'params': shared_params, 'lr': shared_lr, 'weight_decay': weight_decay},  # å…±äº«å±‚ç”¨æ›´å°çš„å­¦ä¹ ç‡
                    {'params': output_params, 'lr': original_lr, 'weight_decay': weight_decay}  # è¾“å‡ºå±‚ç”¨æ­£å¸¸å­¦ä¹ ç‡
                ]
                if attention_params:
                        param_groups.insert(1, {'params': attention_params, 'lr': attention_lr, 'weight_decay': weight_decay})  # æ³¨æ„åŠ›å±‚ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
                
                optimizer = torch.optim.AdamW(param_groups)
                if attention_params:
                    print_report(f'ä¼˜åŒ–å™¨: åŠ¨æ€åˆ†å±‚å­¦ä¹ ç‡ï¼ˆattention_weight={attention_weight:.2f}, fine_tune_stage=fullï¼‰- å…±äº«å±‚ lr={shared_lr:.6f} ({shared_lr_factor:.3f}x), æ³¨æ„åŠ›å±‚ lr={attention_lr:.6f} ({attention_lr_factor:.3f}x), è¾“å‡ºå±‚ lr={original_lr:.6f}')
                else:
                    print_report(f'ä¼˜åŒ–å™¨: åŠ¨æ€åˆ†å±‚å­¦ä¹ ç‡ï¼ˆattention_weight={attention_weight:.2f}, fine_tune_stage=fullï¼‰- å…±äº«å±‚ lr={shared_lr:.6f} ({shared_lr_factor:.3f}x), è¾“å‡ºå±‚ lr={original_lr:.6f}')
            else:
                # å³ä½¿fine_tune_stage == 'none'ï¼Œå¦‚æœä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡ä¿æŠ¤é¢„è®­ç»ƒæƒé‡
                # ä¼˜åŒ–ç­–ç•¥ï¼šæ ¹æ®attention_weightåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
                # attention_weightè¶Šå° â†’ æ³¨æ„åŠ›å½±å“å° â†’ å¯ä»¥æ›´ä¿¡ä»»é¢„è®­ç»ƒæƒé‡ â†’ å…±äº«å±‚å­¦ä¹ ç‡å¯ä»¥ç¨å¤§
                # attention_weightè¶Šå¤§ â†’ æ³¨æ„åŠ›å½±å“å¤§ â†’ éœ€è¦æ›´å°å¿ƒä¿æŠ¤é¢„è®­ç»ƒæƒé‡ â†’ å…±äº«å±‚å­¦ä¹ ç‡åº”è¯¥æ›´å°
                
                # æ£€æŸ¥æ£€æŸ¥ç‚¹ä¸­çš„ä¼˜åŒ–å™¨é…ç½®ï¼Œå¦‚æœæ£€æŸ¥ç‚¹ä½¿ç”¨äº†åˆ†å±‚å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬ä¹Ÿåº”è¯¥ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡
                # ä½¿ç”¨å…¨å±€å˜é‡ checkpoint_uses_layered_lrï¼ˆåœ¨åŠ è½½æ£€æŸ¥ç‚¹æ—¶å·²è®¾ç½®ï¼‰
                if has_checkpoint and checkpoint_uses_layered_lr:
                    print_report(f'âœ“ æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ä½¿ç”¨äº†åˆ†å±‚å­¦ä¹ ç‡ï¼Œå°†åŒ¹é…æ­¤é…ç½®ä»¥æ­£ç¡®åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€')
                
                # å¦‚æœä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œåº”è¯¥ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡ä¿æŠ¤é¢„è®­ç»ƒæƒé‡
                # æ¡ä»¶ï¼šuse_pretrained=True ä¸” (æ²¡æœ‰checkpoint æˆ– checkpointä½¿ç”¨äº†åˆ†å±‚å­¦ä¹ ç‡ æˆ– æ˜¯ä».pthæ–‡ä»¶åŠ è½½)
                # ä».pthæ–‡ä»¶åŠ è½½æ—¶ï¼Œè™½ç„¶æ— æ³•æ£€æµ‹checkpointçš„ä¼˜åŒ–å™¨é…ç½®ï¼Œä½†å¦‚æœä½¿ç”¨äº†é¢„è®­ç»ƒæƒé‡ï¼Œä»åº”ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡
                # ä½†æ˜¯ï¼Œå¦‚æœæ˜ç¡®è®¾ç½®äº† force_layered_lr=Falseï¼Œåˆ™å¼ºåˆ¶ä¸ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡
                if force_layered_lr is False:
                    # æ˜ç¡®ç¦ç”¨åˆ†å±‚å­¦ä¹ ç‡
                    should_use_layered_lr = False
                    print_report('âš™ï¸  force_layered_lr=Falseï¼Œå¼ºåˆ¶ç¦ç”¨åˆ†å±‚å­¦ä¹ ç‡ï¼Œä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡')
                else:
                    should_use_layered_lr = force_layered_lr or (use_pretrained and (not has_checkpoint or checkpoint_uses_layered_lr or is_pth_file))
                
                if force_layered_lr and has_checkpoint and not (checkpoint_uses_layered_lr or is_pth_file):
                    print_report('âš™ï¸  å·²å¯ç”¨ force_layered_lrï¼Œå¿½ç•¥ checkpoint ä¸­çš„å­¦ä¹ ç‡ç­–ç•¥ï¼Œå¼ºåˆ¶ä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡')
                
                if should_use_layered_lr:
                    # è·å–attention_weightï¼ˆé»˜è®¤1.0ï¼‰
                    attention_weight = hyperparameters.get('attention_weight', 1.0)
                    
                    # æ ¹æ®attention_weightåŠ¨æ€è°ƒæ•´å…±äº«å±‚å­¦ä¹ ç‡
                    # attention_weightä»0.1åˆ°0.9ï¼Œå…±äº«å±‚å­¦ä¹ ç‡ä»0.15çº¿æ€§é™ä½åˆ°0.05ï¼ˆæé«˜å­¦ä¹ ç‡ï¼‰
                    # å…¬å¼ï¼šshared_lr_factor = 0.15 - 0.125 * (attention_weight - 0.1) / 0.8
                    # å½“attention_weight=0.1æ—¶ï¼Œshared_lr_factor=0.15
                    # å½“attention_weight=0.9æ—¶ï¼Œshared_lr_factor=0.05
                    if attention_weight <= 0.1:
                        shared_lr_factor = 0.15  # æ³¨æ„åŠ›å½±å“å¾ˆå°ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
                    elif attention_weight >= 0.9:
                        shared_lr_factor = 0.05  # æ³¨æ„åŠ›å½±å“å¾ˆå¤§ï¼Œä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆä½†ä»æ¯”ä¹‹å‰é«˜ï¼‰
                    else:
                        # çº¿æ€§æ’å€¼ï¼š0.1->0.15, 0.9->0.05
                        shared_lr_factor = 0.15 - 0.125 * (attention_weight - 0.1) / 0.8
                    
                    # æ ¹æ®attention_weightåŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›å±‚å­¦ä¹ ç‡
                    # attention_weightè¶Šå°ï¼Œæ³¨æ„åŠ›å±‚éœ€è¦æ›´å¿«å­¦ä¹ ï¼ˆå› ä¸ºå½±å“å°ï¼Œéœ€è¦å¿«é€Ÿé€‚åº”ï¼‰
                    # attention_weightè¶Šå¤§ï¼Œæ³¨æ„åŠ›å±‚å¯ä»¥ç¨æ…¢å­¦ä¹ ï¼ˆå› ä¸ºå½±å“å¤§ï¼Œå·²ç»å ä¸»å¯¼ï¼‰
                    if attention_weight <= 0.1:
                        attention_lr_factor = 2.5  # æ³¨æ„åŠ›å½±å“å°ï¼Œéœ€è¦å¿«é€Ÿå­¦ä¹ é€‚åº”
                    elif attention_weight >= 0.9:
                        attention_lr_factor = 1.5  # æ³¨æ„åŠ›å½±å“å¤§ï¼Œå·²ç»å ä¸»å¯¼ï¼Œå¯ä»¥ç¨æ…¢
                    else:
                        # çº¿æ€§æ’å€¼ï¼š0.1->2.5, 0.9->1.5
                        attention_lr_factor = 2.5 - 1.25 * (attention_weight - 0.1) / 0.8
                    
                    # æ ¹æ®attention_weightåŠ¨æ€è°ƒæ•´è¾“å‡ºå±‚å­¦ä¹ ç‡
                    # attention_weightè¶Šå°ï¼Œè¾“å‡ºå±‚å¯ä»¥ç¨å¤§å­¦ä¹ ç‡ï¼ˆé¢„è®­ç»ƒæƒé‡æ›´å¯é ï¼‰
                    # attention_weightè¶Šå¤§ï¼Œè¾“å‡ºå±‚éœ€è¦æ›´å°å­¦ä¹ ç‡ï¼ˆä¿æŠ¤é¢„è®­ç»ƒæƒé‡ï¼‰
                    # æé«˜å­¦ä¹ ç‡ï¼šä»0.5-0.2æ”¹ä¸º0.8-0.4
                    if attention_weight <= 0.1:
                        output_lr_factor = 0.8  # æ³¨æ„åŠ›å½±å“å°ï¼Œé¢„è®­ç»ƒæƒé‡æ›´å¯é ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå¤§å­¦ä¹ ç‡
                    elif attention_weight >= 0.9:
                        output_lr_factor = 0.4  # æ³¨æ„åŠ›å½±å“å¤§ï¼Œéœ€è¦ä¿æŠ¤ï¼Œä½†ä»æ¯”ä¹‹å‰é«˜
                    else:
                        # çº¿æ€§æ’å€¼ï¼š0.1->0.8, 0.9->0.4
                        output_lr_factor = 0.8 - 0.5 * (attention_weight - 0.1) / 0.8
                    
                    shared_lr = original_lr * shared_lr_factor
                    attention_lr = original_lr * attention_lr_factor
                    output_lr = original_lr * output_lr_factor
                    
                    shared_params = []
                    attention_params = []
                    output_params = []
                    
                    for name, param in model.named_parameters():
                        if any(layer in name for layer in ['graphnet1', 'graphnet2', 'gnorm1', 'gnorm2', 'global_conv1']):
                            shared_params.append(param)
                        elif 'ext_attention' in name and ('Mk' in name or 'Mv' in name):
                            attention_params.append(param)
                        else:
                            output_params.append(param)
                    
                    # æ„å»ºå‚æ•°ç»„
                    param_groups = [
                        {'params': shared_params, 'lr': shared_lr, 'weight_decay': weight_decay},  # å…±äº«å±‚ï¼šä¿æŠ¤é¢„è®­ç»ƒæƒé‡
                        {'params': output_params, 'lr': output_lr, 'weight_decay': weight_decay}  # è¾“å‡ºå±‚ï¼šæ­£å¸¸å­¦ä¹ ç‡
                    ]
                    if attention_params:
                        param_groups.insert(1, {'params': attention_params, 'lr': attention_lr, 'weight_decay': weight_decay})  # æ³¨æ„åŠ›å±‚ï¼šå¿«é€Ÿå­¦ä¹ 
                    
                    optimizer = torch.optim.AdamW(param_groups)
                    if attention_params:
                        print_report(f'ä¼˜åŒ–å™¨: åŠ¨æ€åˆ†å±‚å­¦ä¹ ç‡ï¼ˆattention_weight={attention_weight:.2f}ï¼‰- å…±äº«å±‚ lr={shared_lr:.6f} ({shared_lr_factor:.3f}x), æ³¨æ„åŠ›å±‚ lr={attention_lr:.6f} ({attention_lr_factor:.3f}x), è¾“å‡ºå±‚ lr={output_lr:.6f} ({output_lr_factor:.3f}x)')
                    else:
                        print_report(f'ä¼˜åŒ–å™¨: åŠ¨æ€åˆ†å±‚å­¦ä¹ ç‡ï¼ˆattention_weight={attention_weight:.2f}ï¼‰- å…±äº«å±‚ lr={shared_lr:.6f} ({shared_lr_factor:.3f}x), è¾“å‡ºå±‚ lr={output_lr:.6f} ({output_lr_factor:.3f}x)')
                else:
                    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        
        # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨ï¼ˆæ”¯æŒå‘¨æœŸç­–ç•¥æˆ–è‡ªé€‚åº”ç­–ç•¥ï¼‰
        scheduler = _create_scheduler(optimizer)
        
        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œç°åœ¨åŠ è½½å®ƒ
        if optimizer_state_dict is not None:
            try:
                # æ£€æŸ¥å‚æ•°ç»„æ•°é‡æ˜¯å¦åŒ¹é…
                checkpoint_param_groups = len(optimizer_state_dict.get('param_groups', []))
                current_param_groups = len(optimizer.param_groups)
                
                if checkpoint_param_groups != current_param_groups:
                    print_report(f'è­¦å‘Š: ä¼˜åŒ–å™¨å‚æ•°ç»„æ•°é‡ä¸åŒ¹é… (æ£€æŸ¥ç‚¹: {checkpoint_param_groups}, å½“å‰: {current_param_groups})')
                    print_report('  è¿™é€šå¸¸å‘ç”Ÿåœ¨ä½¿ç”¨äº†ä¸åŒçš„å­¦ä¹ ç‡ç­–ç•¥æ—¶ï¼ˆä¾‹å¦‚ï¼Œåˆ†å±‚å­¦ä¹ ç‡ vs å•ä¸€å­¦ä¹ ç‡ï¼‰')
                    print_report('  å°†è·³è¿‡ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½ï¼Œä½¿ç”¨æ–°åˆå§‹åŒ–çš„ä¼˜åŒ–å™¨')
                else:
                    # å‚æ•°ç»„æ•°é‡åŒ¹é…ï¼Œå°è¯•åŠ è½½
                    optimizer.load_state_dict(optimizer_state_dict)
                    print_report('å·²åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€')
            except Exception as e:
                print_report(f'è­¦å‘Š: åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}')
                print_report('  å¯èƒ½åŸå› : å‚æ•°ç»„é…ç½®ä¸åŒ¹é…ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨äº†ä¸åŒçš„åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥ï¼‰')
                print_report('  å°†ä½¿ç”¨æ–°åˆå§‹åŒ–çš„ä¼˜åŒ–å™¨')
        
        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹çš„è°ƒåº¦å™¨çŠ¶æ€ï¼Œç°åœ¨åŠ è½½å®ƒ
        if scheduler_state_dict is not None:
            try:
                scheduler.load_state_dict(scheduler_state_dict)
                print_report('å·²åŠ è½½è°ƒåº¦å™¨çŠ¶æ€')
            except Exception as e:
                print_report(f'è­¦å‘Š: åŠ è½½è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {e}')
                print_report('å°†ä½¿ç”¨æ–°åˆå§‹åŒ–çš„è°ƒåº¦å™¨')

    # å¦‚æœæ¨¡å‹è¢«é‡æ–°åˆ›å»ºï¼ˆç”±äºTritoné”™è¯¯ï¼‰ï¼Œéœ€è¦é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨å’Œscaler
    if 'need_recreate_optimizer' in locals() and need_recreate_optimizer:
        print_report('')
        print_report('='*60)
        print_report('ã€é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨å’ŒScalerã€‘')
        print_report('='*60)
        print_report('ç”±äºæ¨¡å‹å·²é‡æ–°åˆ›å»ºï¼Œéœ€è¦é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨å’Œscalerä»¥åŒ¹é…æ–°çš„æ¨¡å‹å‚æ•°')
        
        # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„é€»è¾‘ï¼‰
        if freeze_shared_layers:
            trainable_params = [p for p in model.parameters() if p.requires_grad]
            optimizer = torch.optim.AdamW(trainable_params, lr=original_lr, weight_decay=weight_decay)
            print_report('âœ“ ä¼˜åŒ–å™¨å·²é‡æ–°åˆ›å»ºï¼ˆåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼‰')
        else:
            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
            print_report('âœ“ ä¼˜åŒ–å™¨å·²é‡æ–°åˆ›å»ºï¼ˆä¼˜åŒ–æ‰€æœ‰å‚æ•°ï¼‰')
        
        # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨
        scheduler = _create_scheduler(optimizer)
        print_report('âœ“ è°ƒåº¦å™¨å·²é‡æ–°åˆ›å»º')
        
        # é‡æ–°åˆ›å»º GradScalerï¼ˆå¦‚æœä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
        if use_mixed_precision and torch.cuda.is_available():
            scaler = GradScaler('cuda')
            print_report('âœ“ GradScalerå·²é‡æ–°åˆ›å»ºï¼ˆæ··åˆç²¾åº¦è®­ç»ƒï¼‰')
        
        print_report('='*60)
        print_report('')
    
    # è®­ç»ƒå¼€å§‹æç¤º
    print_report('')
    print_report('='*60)
    # è®­ç»ƒé…ç½®ä¿¡æ¯è¾“å‡ºå·²ç¦ç”¨ï¼ˆç”¨æˆ·è¦æ±‚ä¸å†è¾“å‡ºï¼‰
    # print_report('ã€å¼€å§‹è®­ç»ƒã€‘')
    # print_report('='*60)
    # print_report(f'æ€»è®­ç»ƒè½®æ•°: {total_epochs}')
    # print_report(f'èµ·å§‹è½®æ¬¡: {start_epoch}')
    # print_report(f'æ‰¹æ¬¡å¤§å°: {batch_size}')
    # print_report(f'è®­ç»ƒé›†å¤§å°: {len(df)}')
    # if val_loader is not None:
    #     print_report(f'éªŒè¯é›†å¤§å°: {len(val_df)}')
    # print_report(f'æ•°æ®åŠ è½½è¿›ç¨‹æ•°: {num_workers} (å•è¿›ç¨‹æ¨¡å¼ï¼Œå·²ç¦ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ)')
    # print_report(f'æ•°æ®é¢„å–å› å­: {prefetch_factor}')
    # print_report(f'å†…å­˜å›ºå®š: {"å¯ç”¨" if pin_memory else "ç¦ç”¨"} (å·²ç¦ç”¨)')
    # print_report(f'æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹: {"å¯ç”¨" if persistent_workers else "ç¦ç”¨"} (å·²ç¦ç”¨)')
    # print_report(f'æ··åˆç²¾åº¦è®­ç»ƒ: {"å¯ç”¨" if scaler is not None else "ç¦ç”¨"} (å·²ç¦ç”¨)')
    # print_report(f'æ¨¡å‹ç¼–è¯‘: {"å¯ç”¨" if use_torch_compile else "ç¦ç”¨"} (å·²ç¦ç”¨)')
    # print_report(f'æ¨¡å‹é¢„çƒ­: ç¦ç”¨ (å·²å–æ¶ˆæ‰€æœ‰è®­ç»ƒåŠ é€Ÿæ–¹æ³•)')
    # 
    # # è®¡ç®—é¢„æœŸçš„batchæ•°é‡å’Œæ€§èƒ½æŒ‡æ ‡
    # try:
    #     expected_batches = len(train_loader)
    #     samples_per_epoch = batch_size * expected_batches
    #     print_report(f'æ¯ä¸ªepochçš„batchæ•°: {expected_batches}')
    #     print_report(f'æ¯ä¸ªepochçš„æ ·æœ¬æ•°: {samples_per_epoch}')
    #     if expected_batches > 0:
    #         # ä¼°ç®—æ¯ä¸ªbatchçš„ç†æƒ³æ—¶é—´ï¼ˆå‡è®¾GPUåˆ©ç”¨ç‡100%ï¼‰
    #         # å¯¹äºå›¾ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªbatché€šå¸¸éœ€è¦0.1-0.3ç§’ï¼ˆå–å†³äºæ¨¡å‹å¤æ‚åº¦å’Œbatchå¤§å°ï¼‰
    #         estimated_batch_time = 0.15  # ä¿å®ˆä¼°è®¡
    #         estimated_epoch_time = expected_batches * estimated_batch_time
    #         print_report(f'é¢„ä¼°æ¯ä¸ªepochæ—¶é—´: {estimated_epoch_time:.1f}ç§’ï¼ˆç†æƒ³æƒ…å†µï¼Œå®é™…å¯èƒ½æ›´æ…¢ï¼‰')
    # except Exception:
    #     pass
    # 
    # print_report('='*60)
    # print_report('')
    
    # æ¨¡å‹é¢„çƒ­ï¼šå·²ç¦ç”¨ï¼ˆå–æ¶ˆæ‰€æœ‰è®­ç»ƒåŠ é€Ÿæ–¹æ³•ï¼‰
    # å¦‚æœ start_epoch == 0:  # åªåœ¨ä»å¤´è®­ç»ƒæ—¶é¢„çƒ­
    #     print_report('ã€æ¨¡å‹é¢„çƒ­ã€‘é¢„çƒ­æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨ï¼ˆåŠ é€Ÿé¦–æ¬¡è¿­ä»£ï¼‰...')
    if False:  # ç¦ç”¨æ¨¡å‹é¢„çƒ­
        print_report('ã€æ¨¡å‹é¢„çƒ­ã€‘é¢„çƒ­æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨ï¼ˆåŠ é€Ÿé¦–æ¬¡è¿­ä»£ï¼‰...')
        try:
            model.eval()
            warmup_start = time.time()
            with torch.no_grad():
                # è·å–ç¬¬ä¸€ä¸ªbatchè¿›è¡Œé¢„çƒ­
                warmup_iter = iter(train_loader)
                warmup_batch = next(warmup_iter)
                
                if len(warmup_batch) == 3:
                    warmup_solvent, warmup_solute, warmup_T = warmup_batch
                    warmup_T = warmup_T.to(device, non_blocking=True)
                    warmup_solvent = warmup_solvent.to(device, non_blocking=True)
                    warmup_solute = warmup_solute.to(device, non_blocking=True)
                    # è¿è¡Œä¸€æ¬¡forwardé¢„çƒ­
                    _ = model(warmup_solvent, warmup_solute, warmup_T)
                elif len(warmup_batch) == 2:
                    warmup_solvent, warmup_solute = warmup_batch
                    warmup_solvent = warmup_solvent.to(device, non_blocking=True)
                    warmup_solute = warmup_solute.to(device, non_blocking=True)
                    # è¿è¡Œä¸€æ¬¡forwardé¢„çƒ­
                    _ = model(warmup_solvent, warmup_solute)
            
            warmup_time = time.time() - warmup_start
            print_report(f'âœ“ æ¨¡å‹é¢„çƒ­å®Œæˆï¼ˆè€—æ—¶: {warmup_time:.2f}ç§’ï¼‰')
            print_report('   å·²é¢„çƒ­ï¼šCUDAåˆå§‹åŒ–ã€æ¨¡å‹ç¼–è¯‘ã€æ•°æ®åŠ è½½å™¨ã€GPUå†…å­˜åˆ†é…')
            print_report('')
        except Exception as e:
            error_msg = str(e)
            error_type = str(type(e).__name__)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Triton ç›¸å…³é”™è¯¯
            is_triton_error = (
                'triton' in error_msg.lower() or 
                'TritonMissing' in error_type or
                'Cannot find a working triton' in error_msg
            )
            
            if is_triton_error and model_compiled:
                print_report('')
                print_report('='*60)
                print_report('âš ï¸ æ£€æµ‹åˆ° Triton ç›¸å…³é”™è¯¯')
                print_report('='*60)
                print_report('æ¨¡å‹ç¼–è¯‘éœ€è¦ Tritonï¼Œä½†ç³»ç»Ÿæœªå®‰è£…æˆ–æ— æ³•ä½¿ç”¨ Triton')
                print_report('å°†ç¦ç”¨æ¨¡å‹ç¼–è¯‘ï¼Œä½¿ç”¨æœªç¼–è¯‘çš„æ¨¡å‹ç»§ç»­è®­ç»ƒ')
                print_report('='*60)
                print_report('')
                
                # é‡æ–°åˆ›å»ºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨ç¼–è¯‘ï¼‰
                print_report('é‡æ–°åˆ›å»ºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨ç¼–è¯‘ï¼‰...')
                model = GHGEAT(v_in, e_in, u_in, hidden_dim, attention_weight=attention_weight)
                model = model.to(device)
                
                # é‡æ–°åŠ è½½æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
                if has_checkpoint:
                    # é‡æ–°åŠ è½½æ£€æŸ¥ç‚¹æƒé‡
                    checkpoint = torch.load(checkpoint_to_load, map_location=torch.device(available_device), weights_only=False)
                    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                        checkpoint_state = checkpoint['model_state_dict']
                    else:
                        checkpoint_state = checkpoint
                    model.load_state_dict(checkpoint_state, strict=False)
                    print_report('âœ“ å·²é‡æ–°åŠ è½½æ£€æŸ¥ç‚¹æƒé‡')
                elif use_pretrained and os.path.exists(pretrained_path):
                    # é‡æ–°åŠ è½½é¢„è®­ç»ƒæƒé‡
                    checkpoint = torch.load(pretrained_path, map_location=torch.device(available_device), weights_only=False)
                    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                        pretrained_dict = checkpoint['model_state_dict']
                    else:
                        pretrained_dict = checkpoint
                    model_dict = model.state_dict()
                    # ç®€åŒ–çš„æƒé‡åŠ è½½ï¼ˆå¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼‰
                    for key in pretrained_dict.keys():
                        model_key = None
                        if 'shared_layer.' in key:
                            model_key = key.replace('shared_layer.', '')
                        elif 'task_A.' in key:
                            model_key = key.replace('task_A.', '')
                        elif 'task_B.' in key:
                            model_key = key.replace('task_B.', '')
                        else:
                            model_key = key
                        
                        if model_key and model_key in model_dict:
                            if model_dict[model_key].shape == pretrained_dict[key].shape:
                                model_dict[model_key] = pretrained_dict[key]
                    model.load_state_dict(model_dict, strict=False)
                    print_report('âœ“ å·²é‡æ–°åŠ è½½é¢„è®­ç»ƒæƒé‡')
                
                model_compiled = False
                print_report('âœ“ æ¨¡å‹å·²é‡æ–°åˆ›å»ºï¼ˆæœªç¼–è¯‘ï¼‰')
                
                # æ ‡è®°éœ€è¦é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆå› ä¸ºæ¨¡å‹å‚æ•°å·²æ”¹å˜ï¼‰
                need_recreate_optimizer = True
                print_report('   æ³¨æ„ï¼šå°†åœ¨é¢„çƒ­åé‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆå› ä¸ºæ¨¡å‹å·²é‡æ–°åˆ›å»ºï¼‰')
                print_report('')
                
                # é‡æ–°å°è¯•é¢„çƒ­
                try:
                    model.eval()
                    warmup_start = time.time()
                    with torch.no_grad():
                        if len(warmup_batch) == 3:
                            _ = model(warmup_solvent, warmup_solute, warmup_T)
                        elif len(warmup_batch) == 2:
                            _ = model(warmup_solvent, warmup_solute)
                    warmup_time = time.time() - warmup_start
                    print_report(f'âœ“ æ¨¡å‹é¢„çƒ­å®Œæˆï¼ˆè€—æ—¶: {warmup_time:.2f}ç§’ï¼‰')
                    print_report('')
                except Exception as e2:
                    print_report(f'âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e2}')
                    print_report('   å°†è·³è¿‡é¢„çƒ­ï¼Œé¦–æ¬¡è¿­ä»£å¯èƒ½è¾ƒæ…¢')
                    print_report('')
            else:
                print_report(f'âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}')
                print_report('   å°†è·³è¿‡é¢„çƒ­ï¼Œé¦–æ¬¡è¿­ä»£å¯èƒ½è¾ƒæ…¢')
                print_report('')

    # ğŸ”‘ åœ¨è®­ç»ƒå¼€å§‹å‰éªŒè¯è¾“å…¥æ•°æ®ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªepochå’Œstart_epoch==0æ—¶æ£€æŸ¥ï¼‰
    if start_epoch == 0:
        print_report('ã€æ•°æ®éªŒè¯ã€‘æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡...')
        try:
            # æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬
            sample_count = 0
            max_samples_to_check = min(10, len(df))
            for idx in range(max_samples_to_check):
                try:
                    sample_solv = graphs_solv[idx]
                    sample_solu = graphs_solu[idx]
                    # æ£€æŸ¥å›¾æ•°æ®ä¸­çš„ç‰¹å¾
                    if hasattr(sample_solv, 'x') and sample_solv.x is not None:
                        if torch.any(~torch.isfinite(sample_solv.x)):
                            nan_count = torch.sum(~torch.isfinite(sample_solv.x)).item()
                            print_report(f'  âš ï¸ æ ·æœ¬ {idx} æº¶å‰‚èŠ‚ç‚¹ç‰¹å¾åŒ…å«NaN/Inf: {nan_count} ä¸ªå€¼')
                    if hasattr(sample_solv, 'y') and sample_solv.y is not None:
                        if torch.any(~torch.isfinite(sample_solv.y)):
                            print_report(f'  âš ï¸ æ ·æœ¬ {idx} æº¶å‰‚æ ‡ç­¾åŒ…å«NaN/Inf')
                    if hasattr(sample_solu, 'x') and sample_solu.x is not None:
                        if torch.any(~torch.isfinite(sample_solu.x)):
                            nan_count = torch.sum(~torch.isfinite(sample_solu.x)).item()
                            print_report(f'  âš ï¸ æ ·æœ¬ {idx} æº¶è´¨èŠ‚ç‚¹ç‰¹å¾åŒ…å«NaN/Inf: {nan_count} ä¸ªå€¼')
                    sample_count += 1
                except Exception as e:
                    print_report(f'  âš ï¸ æ£€æŸ¥æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}')
            
            if sample_count == max_samples_to_check:
                print_report(f'âœ“ å·²æ£€æŸ¥ {sample_count} ä¸ªæ ·æœ¬ï¼Œæœªå‘ç°æ˜æ˜¾çš„NaN/Infï¼ˆåœ¨èŠ‚ç‚¹ç‰¹å¾ä¸­ï¼‰')
        except Exception as e:
            print_report(f'  âš ï¸ æ•°æ®éªŒè¯å¤±è´¥: {e}')
        print_report('')
    
    for epoch in range(start_epoch, total_epochs):
        epoch_start_time = time.time()
        relative_epoch = epoch - start_epoch + 1
        stats = OrderedDict()
        relative_epoch = epoch - start_epoch + 1
        
        # ç¬¬ä¸€ä¸ªepochå¼€å§‹æ—¶è¾“å‡ºæç¤º
        if epoch == start_epoch:
            print_report(f'å¼€å§‹ç¬¬ {epoch+1} è½®è®­ç»ƒ...')
        
        # ä¸¤é˜¶æ®µå¾®è°ƒï¼šåœ¨æŒ‡å®šepochè§£å†»å…±äº«å±‚
        if use_pretrained and not has_checkpoint and fine_tune_stage == 'two_stage' and freeze_shared_layers:
            if epoch == freeze_epochs:
                # è§£å†»æ‰€æœ‰å±‚ï¼Œåˆ‡æ¢åˆ°å…¨é‡å¾®è°ƒ
                # print_report(f'\n{"="*60}')
                # print_report(f'ã€é˜¶æ®µåˆ‡æ¢ã€‘ç¬¬ {epoch+1} è½®ï¼šè§£å†»æ‰€æœ‰å±‚ï¼Œå¼€å§‹å…¨é‡å¾®è°ƒ')
                # print_report(f'{"="*60}')
                
                # è§£å†»æ‰€æœ‰å‚æ•°
                for name, param in model.named_parameters():
                    param.requires_grad = True
                
                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼Œä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡è¿›è¡Œå…¨é‡å¾®è°ƒ
                fine_tune_lr = original_lr * fine_tune_lr_factor
                reduction_factor_str = f"{1/fine_tune_lr_factor:.1f}å€" if fine_tune_lr_factor < 1 else "ä¸å˜"
                optimizer = torch.optim.AdamW(model.parameters(), lr=fine_tune_lr, weight_decay=weight_decay)
                
                # é‡æ–°åˆ›å»º GradScalerï¼ˆå¦‚æœä½¿ç”¨æ··åˆç²¾åº¦ï¼‰ï¼Œç¡®ä¿ä¸æ–°çš„ optimizer åŒ¹é…
                if use_mixed_precision and torch.cuda.is_available():
                    scaler = GradScaler('cuda')
                    print_report('âœ“ GradScalerå·²é‡æ–°åˆ›å»ºï¼ˆåŒ¹é…æ–°çš„ä¼˜åŒ–å™¨ï¼‰')
                
                # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨ï¼ˆæ”¯æŒé…ç½®çš„å‘¨æœŸç­–ç•¥ï¼‰
                remaining_epochs = total_epochs - epoch
                scheduler = _create_scheduler(optimizer)
                # print_report(f'å…¨é‡å¾®è°ƒå­¦ä¹ ç‡: {fine_tune_lr:.6f} (é™ä½{reduction_factor_str})')
                # print_report(f'å‰©ä½™è®­ç»ƒè½®æ•°: {remaining_epochs}')
                # print_report(f'é˜¶æ®µ2 Warmup: {warmup_remaining} epochs ({warmup_remaining/remaining_epochs*100:.1f}%)')
                # print_report(f'{"="*60}\n')
                
                freeze_shared_layers = False  # æ ‡è®°å·²è§£å†»

        # Trainï¼ˆä½¿ç”¨ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°ï¼Œæ”¯æŒæ··åˆç²¾åº¦ï¼‰
        # éªŒè¯è®¾å¤‡ä½¿ç”¨æƒ…å†µï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªepochæ£€æŸ¥ï¼Œå·²ç¦ç”¨è¾“å‡ºï¼‰
        # if epoch == 0:
        #     # éªŒè¯è®¾å¤‡ï¼ˆå·²ç¦ç”¨è¾“å‡ºï¼‰
        #     # sample_param = next(model.parameters())
        #     # actual_device = sample_param.device
        #     # if torch.cuda.is_available() and actual_device.type != 'cuda':
        #     #     print_report(f'âš ï¸ è­¦å‘Š: æ¨¡å‹å‚æ•°ä¸åœ¨CUDAä¸Šï¼å®é™…è®¾å¤‡: {actual_device}')
        #     # elif torch.cuda.is_available():
        #     #     print_report(f'âœ“ è®­ç»ƒä½¿ç”¨CUDAè®¾å¤‡: {actual_device}')
        #     # else:
        #     #     print_report(f'âš ï¸ è­¦å‘Š: ä½¿ç”¨CPUè®­ç»ƒï¼Œé€Ÿåº¦ä¼šå¾ˆæ…¢')
        #     pass
        
        train_stats = train(model, device, train_loader, optimizer, task_type, stats, scaler=scaler)
        stats.update(train_stats)
        
        # å¦‚æœ train å‡½æ•°é‡æ–°åˆ›å»ºäº† scalerï¼Œæ›´æ–°æœ¬åœ°çš„ scaler å¼•ç”¨
        if '_updated_scaler' in train_stats:
            scaler = train_stats['_updated_scaler']
            del train_stats['_updated_scaler']  # æ¸…ç†ä¸´æ—¶é”®
        
        # éªŒè¯é¢‘ç‡ä¼˜åŒ–ï¼šå¯ä»¥è®¾ç½®æ¯Nä¸ªepochéªŒè¯ä¸€æ¬¡ï¼ˆé»˜è®¤æ¯ä¸ªepochéƒ½éªŒè¯ï¼‰
        eval_interval = hyperparameters.get('eval_interval', 1)  # 1è¡¨ç¤ºæ¯ä¸ªepochéƒ½éªŒè¯
        should_eval = (epoch % eval_interval == 0) or (epoch == total_epochs - 1)  # æœ€åä¸€ä¸ªepochæ€»æ˜¯éªŒè¯
        
        if should_eval:
            # Evaluation on training set
            stats.update(eval(model, device, train_loader, MAE, stats, 'Train', task_type))
            stats.update(eval(model, device, train_loader, R2, stats, 'Train', task_type))
        
            # Evaluation on validation set (if provided)
            eval_loader = val_loader if val_loader is not None else train_loader
            eval_split_label = 'Valid' if val_loader is not None else 'Train'
            stats.update(eval(model, device, eval_loader, MAE, stats, eval_split_label, task_type))
            stats.update(eval(model, device, eval_loader, R2, stats, eval_split_label, task_type))
        else:
            # å³ä½¿ä¸è¿›è¡Œå®Œæ•´éªŒè¯ï¼Œä¹Ÿéœ€è¦è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡ç”¨äºç›‘æ§
            stats.update(eval(model, device, train_loader, MAE, stats, 'Train', task_type))
            stats.update(eval(model, device, train_loader, R2, stats, 'Train', task_type))
            # å¯¹äºéªŒè¯é›†ï¼Œä½¿ç”¨ä¸Šä¸€æ¬¡çš„å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if val_loader is not None and len(mae_valid) > 0:
                # ä½¿ç”¨ä¸Šä¸€æ¬¡çš„éªŒè¯é›†æŒ‡æ ‡
                stats[f'MAE_Valid'] = mae_valid[-1]
                stats[f'R2_Valid'] = r2_valid[-1] if len(r2_valid) > 0 else 0.0
            eval_loader = val_loader if val_loader is not None else train_loader
            eval_split_label = 'Valid' if val_loader is not None else 'Train'
        
        # ğŸ”‘ NaN/Infæ£€æµ‹ï¼šæ£€æŸ¥è®­ç»ƒæŒ‡æ ‡æ˜¯å¦æœ‰æ•ˆ
        train_mae = stats.get('MAE_Train', np.inf)
        train_r2 = stats.get('R2_Train', -np.inf)
        valid_mae = stats.get(f'MAE_{eval_split_label}', np.inf)
        valid_r2 = stats.get(f'R2_{eval_split_label}', -np.inf)
        
        # æ£€æµ‹å¼‚å¸¸å€¼
        has_nan_or_inf = (
            not np.isfinite(train_mae) or 
            not np.isfinite(train_r2) or 
            not np.isfinite(valid_mae) or 
            not np.isfinite(valid_r2)
        )
        
        if has_nan_or_inf:
            print_report('')
            print_report('='*60)
            print_report(f'âŒ è®­ç»ƒå¤±è´¥ï¼šæ£€æµ‹åˆ°NaNæˆ–Infå€¼ (Epoch {epoch+1})')
            print_report(f'   Train MAE: {train_mae:.6f}, RÂ²: {train_r2:.6f}')
            print_report(f'   {eval_split_label} MAE: {valid_mae:.6f}, RÂ²: {valid_r2:.6f}')
            
            # ğŸ”‘ æ·»åŠ è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
            print_report('')
            print_report('ã€è¯¦ç»†è¯Šæ–­ä¿¡æ¯ã€‘')
            print_report(f'   è®­ç»ƒMAEæ˜¯å¦æœ‰é™: {np.isfinite(train_mae)}')
            print_report(f'   è®­ç»ƒRÂ²æ˜¯å¦æœ‰é™: {np.isfinite(train_r2)}')
            print_report(f'   éªŒè¯MAEæ˜¯å¦æœ‰é™: {np.isfinite(valid_mae)}')
            print_report(f'   éªŒè¯RÂ²æ˜¯å¦æœ‰é™: {np.isfinite(valid_r2)}')
            
            # æ£€æŸ¥æ¨¡å‹å‚æ•°çš„å½“å‰èŒƒå›´
            try:
                max_weight = 0.0
                max_weight_name = ''
                for name, param in model.named_parameters():
                    if param.requires_grad:
                        param_max = param.data.abs().max().item()
                        if param_max > max_weight:
                            max_weight = param_max
                            max_weight_name = name
                print_report(f'   æœ€å¤§æƒé‡ç»å¯¹å€¼: {max_weight:.2f} (åœ¨ {max_weight_name})')
                if max_weight > 1000:
                    print_report(f'   âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸å¤§çš„æƒé‡å€¼ï¼Œå¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®š')
            except Exception as e:
                print_report(f'   æ— æ³•æ£€æŸ¥æ¨¡å‹å‚æ•°: {e}')
            
            print_report('')
            print_report('   å¯èƒ½åŸå› ï¼š')
            print_report('   1. æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Infï¼ˆå·²åœ¨æ¨¡å‹å†…éƒ¨å’Œè®­ç»ƒå¾ªç¯ä¸­å¤„ç†ï¼‰')
            print_report('   2. RÂ²è®¡ç®—æ—¶é‡åˆ°é—®é¢˜ï¼ˆçœŸå®å€¼æ–¹å·®ä¸º0æˆ–æ•°æ®é—®é¢˜ï¼‰')
            print_report('   3. æ¨¡å‹æƒé‡è¿‡å¤§å¯¼è‡´è¾“å‡ºæº¢å‡º')
            print_report('   4. è¾“å…¥æ•°æ®æœ¬èº«åŒ…å«å¼‚å¸¸å€¼')
            print_report('')
            print_report('   å»ºè®®ï¼š')
            print_report('   1. æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦åŒ…å«NaNæˆ–å¼‚å¸¸å€¼')
            print_report('   2. æ£€æŸ¥æ¨¡å‹æƒé‡åˆå§‹åŒ–')
            print_report('   3. å°è¯•æ›´å°çš„å­¦ä¹ ç‡ï¼ˆè™½ç„¶å·²å°è¯•ä½†å¯èƒ½ä»éœ€æ›´å°ï¼‰')
            print_report('   4. æ£€æŸ¥æ•°æ®é¢„å¤„ç†å’Œå½’ä¸€åŒ–')
            print_report('='*60)
            print_report('')
            # å¯¹äºè¶…å‚æœç´¢ï¼šæ ‡è®°ä¸ºTrialPrunedä»¥è·³è¿‡æ­¤ç»„åˆï¼›å¦åˆ™å›é€€ä¸ºValueError
            try:
                import optuna
                raise optuna.exceptions.TrialPruned(f'è®­ç»ƒåœ¨epoch {epoch+1}å‡ºç°NaN/Infï¼Œå­¦ä¹ ç‡å¯èƒ½è¿‡å¤§æˆ–æ¨¡å‹ä¸ç¨³å®š')
            except ImportError:
                raise ValueError(f'è®­ç»ƒåœ¨epoch {epoch+1}å‡ºç°NaN/Infï¼Œå­¦ä¹ ç‡å¯èƒ½è¿‡å¤§æˆ–æ¨¡å‹ä¸ç¨³å®š')
        
        # ç”¨äºæ—©åœå’Œè°ƒåº¦çš„MAEï¼ˆä¼˜å…ˆä½¿ç”¨éªŒè¯é›†ï¼‰
        # å¦‚æœå½“å‰epochæ²¡æœ‰éªŒè¯ï¼Œä½¿ç”¨ä¸Šä¸€æ¬¡çš„éªŒè¯é›†MAE
        if should_eval:
            mae_for_scheduler = stats[f'MAE_{eval_split_label}']
        else:
            # ä½¿ç”¨ä¸Šä¸€æ¬¡çš„éªŒè¯é›†MAEï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨è®­ç»ƒé›†MAE
            if val_loader is not None and len(mae_valid) > 0:
                mae_for_scheduler = mae_valid[-1]
            else:
                mae_for_scheduler = stats['MAE_Train']
        
        # Schedulerï¼ˆæ ¹æ®é…ç½®å¯èƒ½æ˜¯å‘¨æœŸè°ƒåº¦å™¨æˆ–ReduceLROnPlateauï¼‰
        if scheduler_type == 'plateau':
            scheduler.step(mae_for_scheduler)
        else:
            scheduler.step()
        # Early stoppingï¼ˆä½¿ç”¨éªŒè¯é›†MAEï¼Œå¦‚æœæä¾›ï¼‰
        if early_stop_active:
            early_stopper.step(mae_for_scheduler)
            if early_stopper.should_stop:
                # ä½¿ç”¨å®Œæ•´çš„éªŒè¯é›†å†å²æ¥æŠ¥å‘ŠçœŸå®æœ€ä½³è½®æ¬¡ï¼Œé¿å…ä¸æ—©åœè®¡æ•°ä¸ä¸€è‡´
                if val_loader is not None and len(mae_valid) > 0:
                    best_valid_mae = min(mae_valid)
                    best_valid_epoch = mae_valid.index(best_valid_mae) + 1
                else:
                    best_valid_mae = early_stopper.best
                    best_valid_epoch = epoch + 1 - early_stopper.num_bad
                print_report(f'æ—©åœè§¦å‘ï¼š{eval_split_label} MAEåœ¨{early_stopper.num_bad}è½®å†…æœªæ˜¾è‘—æ”¹å–„ï¼ˆpatience={early_stop_resume_patience}, min_delta={early_stop_min_delta}ï¼‰')
                print_report(f'å…¨ç¨‹æœ€ä½³{eval_split_label} MAE: {best_valid_mae:.6f} (epoch {best_valid_epoch})')
                break
        # Save info
        mae_train.append(stats['MAE_Train'])
        r2_train.append(stats['R2_Train'])
        if val_loader is not None:
            if should_eval:
                mae_valid.append(stats['MAE_Valid'])
                r2_valid.append(stats['R2_Valid'])
            else:
                # å¦‚æœå½“å‰epochæ²¡æœ‰éªŒè¯ï¼Œä½¿ç”¨ä¸Šä¸€æ¬¡çš„å€¼
                if len(mae_valid) > 0:
                    mae_valid.append(mae_valid[-1])
                    r2_valid.append(r2_valid[-1])
                else:
                    # å¦‚æœæ²¡æœ‰å†å²å€¼ï¼Œä½¿ç”¨è®­ç»ƒé›†æŒ‡æ ‡ï¼ˆä¸´æ—¶ï¼‰
                    mae_valid.append(stats['MAE_Train'])
                    r2_valid.append(stats['R2_Train'])
        
        # ä».pthæ–‡ä»¶åŠ è½½æ—¶ï¼Œå½“MAEè¾¾åˆ°é˜ˆå€¼åè‡ªåŠ¨é™ä½å­¦ä¹ ç‡
        if is_pth_file and not lr_reduced:
            mae_threshold = hyperparameters.get('mae_threshold_for_lr_reduction', None)
            fine_tune_lr = hyperparameters.get('fine_tune_lr', None)
            if mae_threshold is not None and fine_tune_lr is not None:
                current_mae = mae_for_scheduler  # ä½¿ç”¨éªŒè¯é›†MAEï¼ˆå¦‚æœæä¾›ï¼‰
                if current_mae <= mae_threshold:
                    print_report(f'\n{"="*60}')
                    print_report(f'ã€å­¦ä¹ ç‡è°ƒæ•´ã€‘{eval_split_label} MAEè¾¾åˆ°é˜ˆå€¼ {mae_threshold:.6f}ï¼ˆå½“å‰MAE: {current_mae:.6f}ï¼‰')
                    print_report(f'å°†å­¦ä¹ ç‡ä» {original_lr:.6f} é™ä½è‡³ {fine_tune_lr:.6f} è¿›è¡Œç²¾ç»†å¾®è°ƒ')
                    print_report(f'{"="*60}\n')
                    
                    # æ›´æ–°æ‰€æœ‰å‚æ•°ç»„çš„å­¦ä¹ ç‡
                    for param_group in optimizer.param_groups:
                        param_group['lr'] = fine_tune_lr
                    
                    lr_reduced = True
                    # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨ä»¥é€‚åº”æ–°çš„å­¦ä¹ ç‡
                    scheduler = _create_scheduler(optimizer)
                    print_report(f'å·²åˆ‡æ¢åˆ°ç²¾ç»†å¾®è°ƒæ¨¡å¼ï¼Œå­¦ä¹ ç‡: {fine_tune_lr:.6f}')
        
        # Save best modelï¼ˆä½¿ç”¨éªŒè¯é›†MAEï¼Œå¦‚æœæä¾›ï¼‰
        if mae_for_scheduler < best_MAE:
            best_model = copy.deepcopy(model.state_dict())
            best_MAE = mae_for_scheduler
            
            # ğŸ”‘ å¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œä¸”æ‰¾åˆ°äº†æ¯”æ¢å¤æ—¶æ›´å¥½çš„æ¨¡å‹ï¼Œåˆ™æ¢å¤æ—©åœ
            if resumed_from_checkpoint and resumed_best_mae is not None:
                # å¦‚æœæ¢å¤æ—¶çš„æœ€ä½³MAEæ˜¯infï¼ˆè¡¨ç¤ºæ²¡æœ‰éªŒè¯é›†å†å²ï¼‰ï¼Œæˆ–è€…æ‰¾åˆ°äº†æ›´å¥½çš„æ¨¡å‹ï¼Œåˆ™æ¢å¤æ—©åœ
                if resumed_best_mae == np.inf or mae_for_scheduler < resumed_best_mae:
                    # æ‰¾åˆ°äº†æ¯”æ¢å¤æ—¶æ›´å¥½çš„æ¨¡å‹ï¼ˆæˆ–é¦–æ¬¡æ›´æ–°ï¼‰ï¼Œæ¢å¤æ—©åœ
                    early_stop_active = True
                    early_stopper = EarlyStopping(patience=early_stop_resume_patience, min_delta=early_stop_min_delta)
                    # é‡ç½®æ—©åœå™¨çŠ¶æ€ï¼Œä»¥æ–°çš„æœ€ä½³MAEä¸ºåŸºå‡†
                    early_stopper.best = mae_for_scheduler
                    early_stopper.num_bad = 0
                    early_stopper.should_stop = False
                    if resumed_best_mae == np.inf:
                        print_report(f'âœ“  Best {eval_split_label} MAE é¦–æ¬¡æ›´æ–°: {best_MAE:.6f} â†’ å·²æ¢å¤æ—©åœ (patience={early_stop_resume_patience})')
                    else:
                        print_report(f'âœ“  Best {eval_split_label} MAE æ›´æ–°: {resumed_best_mae:.6f} â†’ {best_MAE:.6f} â†’ å·²æ¢å¤æ—©åœ (patience={early_stop_resume_patience})')
                    # æ¸…é™¤æ¢å¤æ ‡å¿—ï¼Œåç»­æ­£å¸¸ä½¿ç”¨æ—©åœ
                    resumed_from_checkpoint = False
                    resumed_best_mae = None
                else:
                    # è™½ç„¶æ›´æ–°äº†best_MAEï¼Œä½†è¿˜æ²¡æœ‰è¶…è¿‡æ¢å¤æ—¶çš„æœ€ä½³å€¼ï¼Œç»§ç»­ç­‰å¾…
                    print_report(f'âœ“  Best {eval_split_label} MAE æ›´æ–°: {best_MAE:.6f} (ä»éœ€ç­‰å¾…è¶…è¿‡æ¢å¤æ—¶çš„æœ€ä½³å€¼ {resumed_best_mae:.6f})')
            elif not early_stop_active:
                # éæ£€æŸ¥ç‚¹æ¢å¤æƒ…å†µï¼Œæ­£å¸¸å¯ç”¨æ—©åœ
                early_stop_active = True
                early_stopper = EarlyStopping(patience=early_stop_resume_patience, min_delta=early_stop_min_delta)
                print_report(f'âœ“  Best {eval_split_label} MAE æ›´æ–°: {best_MAE:.6f} â†’ é‡æ–°å¯ç”¨æ—©åœ (patience={early_stop_resume_patience})')
        
        # è®¡ç®—æœ¬è½®è®­ç»ƒæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        
        # åœ¨ä¸€è¡Œä¸­æ˜¾ç¤ºæ¯è½®è®­ç»ƒä¿¡æ¯ï¼ˆåŒ…å«è®­ç»ƒæ—¶é—´ï¼‰
        if val_loader is not None:
            train_mae = stats["MAE_Train"]
            # å¦‚æœå½“å‰epochè¿›è¡Œäº†éªŒè¯ï¼Œä½¿ç”¨æ–°çš„å€¼ï¼›å¦åˆ™ä½¿ç”¨ä¸Šä¸€æ¬¡çš„å€¼
            if should_eval:
                valid_mae = stats["MAE_Valid"]
                valid_r2 = stats["R2_Valid"]
            else:
                valid_mae = mae_valid[-1] if len(mae_valid) > 0 else train_mae
                valid_r2 = r2_valid[-1] if len(r2_valid) > 0 else stats["R2_Train"]
            
            mae_gap = train_mae - valid_mae  # è®­ç»ƒé›†MAE - éªŒè¯é›†MAEï¼Œè´Ÿå€¼è¡¨ç¤ºè¿‡æ‹Ÿåˆ
            gap_warning = ""
            if mae_gap < -0.01:  # å¦‚æœè®­ç»ƒé›†MAEæ¯”éªŒè¯é›†MAEå°è¶…è¿‡0.01ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ
                gap_warning = f" âš ï¸è¿‡æ‹Ÿåˆé£é™©(å·®è·:{mae_gap:.6f})"
            elif mae_gap > 0.01:  # å¦‚æœè®­ç»ƒé›†MAEæ¯”éªŒè¯é›†MAEå¤§ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ
                gap_warning = f" âš ï¸æ¬ æ‹Ÿåˆ(å·®è·:{mae_gap:.6f})"
            
            # ç¡®ä¿best_MAEæ˜¾ç¤ºçš„æ˜¯éªŒè¯é›†æœ€ä½³MAEï¼ˆä»éªŒè¯é›†å†å²ä¸­è·å–ï¼‰
            if len(mae_valid) > 0:
                actual_best_valid_mae = min(mae_valid)
                # å¦‚æœbest_MAEä¸éªŒè¯é›†å†å²ä¸ä¸€è‡´ï¼Œä½¿ç”¨éªŒè¯é›†å†å²æœ€ä½³å€¼
                if abs(best_MAE - actual_best_valid_mae) > 1e-6:
                    best_MAE = actual_best_valid_mae
            
            eval_marker = "" if should_eval else " (è·³è¿‡éªŒè¯)"
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            try:
                expected_batches = len(train_loader)
                if expected_batches > 0:
                    time_per_batch = epoch_time / expected_batches
                    samples_per_second = (batch_size * expected_batches) / epoch_time
                    print_report(f'Epoch {epoch+1}/{total_epochs} - Time: {epoch_time:.2f}s ({time_per_batch:.3f}s/batch, {samples_per_second:.1f} samples/s) - Train MAE: {train_mae:.6f}, RÂ²: {stats["R2_Train"]:.6f} | Valid MAE: {valid_mae:.6f}, RÂ²: {valid_r2:.6f} | Best Valid MAE: {best_MAE:.6f}{gap_warning}{eval_marker}')
                else:
                    print_report(f'Epoch {epoch+1}/{total_epochs} - Time: {epoch_time:.2f}s - Train MAE: {train_mae:.6f}, RÂ²: {stats["R2_Train"]:.6f} | Valid MAE: {valid_mae:.6f}, RÂ²: {valid_r2:.6f} | Best Valid MAE: {best_MAE:.6f}{gap_warning}{eval_marker}')
            except Exception:
                print_report(f'Epoch {epoch+1}/{total_epochs} - Time: {epoch_time:.2f}s - Train MAE: {train_mae:.6f}, RÂ²: {stats["R2_Train"]:.6f} | Valid MAE: {valid_mae:.6f}, RÂ²: {valid_r2:.6f} | Best Valid MAE: {best_MAE:.6f}{gap_warning}{eval_marker}')
        else:
            print_report(f'Epoch {epoch+1}/{total_epochs} - Time: {epoch_time:.2f}s - MAE: {stats["MAE_Train"]:.6f}, RÂ²: {stats["R2_Train"]:.6f} | Best MAE: {best_MAE:.6f}')
        
        # ç¬¬ä¸€è½®MAEé˜ˆå€¼æ£€æŸ¥å·²ç¦ç”¨ï¼ˆé»˜è®¤é˜ˆå€¼ä¸ºæ— ç©·å¤§ï¼Œä¸ä¼šè§¦å‘æå‰ç»ˆæ­¢ï¼‰
        # å¦‚æœéœ€è¦å¯ç”¨ï¼Œå¯ä»¥åœ¨hyperparametersä¸­è®¾ç½® 'first_epoch_mae_threshold' å‚æ•°
        if epoch == 0:  # ç¬¬ä¸€è½®epochå®Œæˆå
            first_epoch_mae_threshold = hyperparameters.get('first_epoch_mae_threshold', float('inf'))  # é»˜è®¤ç¦ç”¨ï¼šè®¾ç½®ä¸ºæ— ç©·å¤§
            first_epoch_mae = mae_for_scheduler  # ä½¿ç”¨éªŒè¯é›†MAEï¼ˆå¦‚æœæä¾›ï¼‰
            
            if first_epoch_mae > first_epoch_mae_threshold:
                print_report(f'âš ï¸ ç¬¬ä¸€è½®{eval_split_label} MAE ({first_epoch_mae:.6f}) è¶…è¿‡é˜ˆå€¼ ({first_epoch_mae_threshold:.6f})ï¼Œæå‰ç»ˆæ­¢è¯•éªŒ')
                print_report(f'   åŸºäºå†å²æ•°æ®åˆ†æï¼Œç¬¬ä¸€è½®MAE > {first_epoch_mae_threshold:.6f} çš„è¯•éªŒé€šå¸¸æ— æ³•è¾¾åˆ°å¥½çš„ç»“æœ')
                raise FirstEpochMAEThresholdExceeded(first_epoch_mae, first_epoch_mae_threshold)
        
        # Save checkpoint
        try:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_model_state_dict': best_model,
                'best_MAE': best_MAE,
                'mae_train': mae_train,
                'r2_train': r2_train,
                'mae_valid': mae_valid,  # ğŸ”‘ æ·»åŠ éªŒè¯é›†MAEå†å²
                'r2_valid': r2_valid,    # ğŸ”‘ æ·»åŠ éªŒè¯é›†RÂ²å†å²
                'early_stopping_state': {
                    'best': early_stopper.best,
                    'num_bad': early_stopper.num_bad,
                    'should_stop': early_stopper.should_stop
                },
                'hyperparameters': hyperparameters
            }
            if (epoch + 1) % checkpoint_interval == 0:
                torch.save(checkpoint, checkpoint_path)
                print_report(f'âœ“  è½®æ¬¡æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}')
                epoch_checkpoint_path = os.path.join(checkpoint_dir, f'{model_name}_checkpoint_epoch{epoch+1:04d}.pth')
                torch.save(checkpoint, epoch_checkpoint_path)
                print_report(f'âœ“  è½®æ¬¡ç¼–å·æ£€æŸ¥ç‚¹å·²ä¿å­˜: {epoch_checkpoint_path}')
        except Exception as e:
            print_report(f'è­¦å‘Š: ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}')

        # æ¯ checkpoint_interval è½®sleep 0.1 ç§’
        should_restart_scheduler = False
        restart_labels = []
        if scheduler_restart_epochs_relative and relative_epoch in scheduler_restart_epochs_relative:
            should_restart_scheduler = True
            restart_labels.append('relative')
        if scheduler_restart_epochs_absolute and (epoch + 1) in scheduler_restart_epochs_absolute:
            should_restart_scheduler = True
            restart_labels.append('absolute')
        if should_restart_scheduler:
            suffix = '_'.join(sorted(set(restart_labels)))
            print_report(f'>>> è§¦å‘è°ƒåº¦å™¨é‡å¯ï¼ˆ{suffix}ï¼‰ä»¥æ¢å¤å‘¨æœŸæ‰°åŠ¨')
            base_lr = hyperparameters.get('lr', lr)
            for param_group in optimizer.param_groups:
                param_group['lr'] = base_lr
            scheduler = _create_scheduler(optimizer)
            _reset_optimizer_momentum(optimizer)
            print_report('âœ“ è°ƒåº¦å™¨å·²é‡å»ºï¼Œå‡†å¤‡è¿›å…¥ä¸‹ä¸€è½®å‘¨æœŸ')
            early_stop_active = False
            early_stopper = EarlyStopping(patience=early_stop_resume_patience, min_delta=early_stop_min_delta)
            print_report(f'>>> æ—©åœå·²ç¦ç”¨ï¼Œå¾…æ›´æ–° Best MAE åå†æ¢å¤ (patience={early_stop_resume_patience})')
        if checkpoint_interval > 0 and (epoch + 1) % checkpoint_interval == 0:
            time.sleep(0.1)

    print_report('-' * 30)
    if val_loader is not None and len(mae_valid) > 0:
        # å¦‚æœæœ‰éªŒè¯é›†ï¼Œæ˜¾ç¤ºéªŒè¯é›†æœ€ä½³ç»“æœï¼ˆä»éªŒè¯é›†å†å²ä¸­è·å–çœŸå®æœ€ä½³å€¼ï¼‰
        actual_best_valid_mae = min(mae_valid)
        best_valid_epoch = mae_valid.index(actual_best_valid_mae) + 1
        best_valid_epoch_idx = best_valid_epoch - 1
        # ç¡®ä¿best_MAEä¸éªŒè¯é›†å†å²æœ€ä½³å€¼ä¸€è‡´
        if abs(best_MAE - actual_best_valid_mae) > 1e-6:
            print_report(f'âš ï¸  æ³¨æ„: best_MAEå˜é‡({best_MAE:.6f})ä¸éªŒè¯é›†å†å²æœ€ä½³({actual_best_valid_mae:.6f})ä¸ä¸€è‡´ï¼Œä½¿ç”¨éªŒè¯é›†å†å²æœ€ä½³å€¼')
            best_MAE = actual_best_valid_mae
        print_report(f'æœ€ä½³éªŒè¯é›†MAE: {best_MAE:.6f} (epoch {best_valid_epoch})')
        print_report(f'éªŒè¯é›†RÂ²      : {r2_valid[best_valid_epoch_idx]:.6f}')
        print_report('-' * 30)
        if len(mae_train) > 0:
            best_train_epoch = mae_train.index(min(mae_train)) + 1
            best_train_epoch_idx = best_train_epoch - 1
            print_report('æœ€ä½³è®­ç»ƒé›†MAE: ' + str(mae_train[best_train_epoch_idx]) + f' (epoch {best_train_epoch})')
            print_report('è®­ç»ƒé›†RÂ²      : ' + str(r2_train[best_train_epoch_idx]))
            best_epoch_idx = best_valid_epoch_idx  # ç”¨äºåç»­ä¿å­˜
        else:
            print_report('è­¦å‘Š: æ²¡æœ‰è®­ç»ƒæ•°æ®')
            best_epoch_idx = best_valid_epoch_idx
    else:
        # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œæ˜¾ç¤ºè®­ç»ƒé›†æœ€ä½³ç»“æœ
        if len(mae_train) > 0:
            best_epoch = mae_train.index(min(mae_train)) + 1
            best_epoch_idx = best_epoch - 1
            print_report('Best Epoch     : ' + str(best_epoch))
            print_report('Training MAE   : ' + str(mae_train[best_epoch_idx]))
            print_report('Training RÂ²    : ' + str(r2_train[best_epoch_idx]))
        else:
            print_report('è­¦å‘Š: æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œæ— æ³•ç¡®å®šæœ€ä½³epoch')
            best_epoch_idx = -1

    # Save training trajectory
    try:
        df_model_training = pd.DataFrame()
        if len(mae_train) > 0:
            df_model_training['MAE_Train'] = mae_train
            df_model_training['R2_Train'] = r2_train
        else:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºç©ºçš„DataFrame
            df_model_training['MAE_Train'] = []
            df_model_training['R2_Train'] = []
        
        # ç¡®ä¿è·¯å¾„å­˜åœ¨ï¼ˆä¿å­˜åˆ°è®­ç»ƒæ–‡ä»¶ç›®å½•ï¼‰
        save_path = training_files_dir if custom_save_path else path
        if not os.path.exists(save_path):
            os.makedirs(save_path, exist_ok=True)
        
        save_train_traj(save_path, df_model_training, valid=False)
        print_report('âœ“ è®­ç»ƒè½¨è¿¹å·²ä¿å­˜: Training.csv')
    except Exception as e:
        print_report(f'è­¦å‘Š: ä¿å­˜è®­ç»ƒè½¨è¿¹å¤±è´¥: {e}')
        import traceback
        traceback.print_exc()

    # Save best modelï¼ˆä¿å­˜åˆ°è®­ç»ƒæ–‡ä»¶ç›®å½•ï¼‰
    try:
        # ç¡®ä¿è·¯å¾„å­˜åœ¨
        save_path = training_files_dir if custom_save_path else path
        if not os.path.exists(save_path):
            os.makedirs(save_path, exist_ok=True)
        
        # å¦‚æœbest_modelä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€
        if best_model is None:
            print_report('è­¦å‘Š: best_modelä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€ä¿å­˜')
            best_model = model.state_dict()
        
        # ä½¿ç”¨os.path.joinç¡®ä¿è·¯å¾„æ­£ç¡®
        model_pth_path = os.path.join(save_path, f'{model_name}.pth')
        torch.save(best_model, model_pth_path)
        print_report(f'âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_pth_path}')
    except Exception as e:
        print_report(f'é”™è¯¯: ä¿å­˜æœ€ä½³æ¨¡å‹å¤±è´¥: {e}')
        import traceback
        traceback.print_exc()
    
    # è®­ç»ƒå®Œæˆåè¯„ä¼°æµ‹è¯•é›†ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    if test_eval_path:
        print_report('-' * 60)
        print_report('ã€æµ‹è¯•é›†è¯„ä¼°ã€‘è®­ç»ƒå®Œæˆï¼Œå¼€å§‹è¯„ä¼°æµ‹è¯•é›†...')
        try:
            if test_df_cache is None:
                test_df_cache = pd.read_csv(test_eval_path)
            test_mae, test_r2, test_predictions, test_targets = evaluate_on_testset(
                model,
                test_df=test_df_cache,
                device=device,
                batch_size=test_eval_batch,
                subset_size=test_eval_subset
            )
            print_report(f'æµ‹è¯•é›†è¯„ä¼°ç»“æœ:')
            print_report(f'  - æµ‹è¯•é›†MAE: {test_mae:.6f}')
            print_report(f'  - æµ‹è¯•é›†RÂ²: {test_r2:.6f}')
            print_report(f'  - æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_predictions)}')
            print_report('-' * 60)
        except Exception as e:
            print_report(f'è­¦å‘Š: æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
        traceback.print_exc()
        # å°è¯•ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€ä½œä¸ºå¤‡é€‰
        try:
            model_pth_path = os.path.join(path, f'{model_name}_fallback.pth')
            torch.save(model.state_dict(), model_pth_path)
            print_report(f'å·²ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€ä½œä¸ºå¤‡é€‰: {model_pth_path}')
        except Exception as e2:
            print_report(f'é”™è¯¯: ä¿å­˜å¤‡é€‰æ¨¡å‹ä¹Ÿå¤±è´¥: {e2}')

    end = time.time()

    print_report('\nTraining time (min): ' + str((end - start) / 60))
    report.close()
if __name__ == '__main__':
    hyperparameters_dict = {'hidden_dim'  : 38,
                            'lr'          : 0.0012947540158123575,
                            'n_epochs'    : 500,
                            'batch_size'  : 64,
                            'fine_tune_stage': 'none',
                            'use_pretrained': True  
                            }
    
    df = pd.read_csv('data\\processed\\new_dataset\\train_dataset\\v2\\molecule\\molecule_train.csv')

    # æ··åˆBrouwer_2021æ•°æ®ï¼ˆ10%-20%ï¼Œé»˜è®¤15%ï¼‰
    # è®¾ç½® mix_brouwer_ratio=0.15 è¡¨ç¤ºæ··åˆ15%çš„Brouwer_2021æ•°æ®
    # å¯ä»¥è®¾ç½®ä¸º 0.1 (10%), 0.15 (15%), 0.2 (20%) æˆ– None (ä¸æ··åˆ)
    # mix_ratio = 0.15  # æ··åˆæ¯”ä¾‹ï¼š0.1=10%, 0.15=15%, 0.2=20%
    # df = mix_brouwer_data(df, brouwer_path='data/raw/Brouwer_2021.csv', mix_ratio=mix_ratio, random_seed=42)

    train_GNNGH_T(df, '1124_GHGEAT', hyperparameters_dict)
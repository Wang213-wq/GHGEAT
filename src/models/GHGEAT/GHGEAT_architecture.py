import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim
import torch.utils.data
import torch_geometric.nn as gnn
from torch_geometric.data import Data
from torch_geometric.nn import global_mean_pool
from torch_geometric.utils import to_dense_adj
from torch_scatter import scatter_add, scatter_mean

from scr.models.utilities_v2.mol2graph import cat_dummy_graph

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class ExternalAttentionLayer(nn.Module):
    def __init__(self, in_features, hidden_dim, u_in, memory_size):
        super(ExternalAttentionLayer, self).__init__()
        self.memory_size = memory_size
        self.in_features = in_features
        self.hidden_dim = hidden_dim

        # å¤–éƒ¨è®°å¿†çŸ©é˜µ Mk å’Œ Mv
        self.Mk = nn.Parameter(torch.randn(memory_size, hidden_dim * 2 + u_in))
        self.Mv = nn.Parameter(torch.randn(memory_size, hidden_dim * 2 + u_in))
        # æ³¨æ„ï¼šä¸åœ¨æ­¤å¤„å®ä¾‹åŒ– nn.Softmaxï¼›åœ¨ forward ä¸­ä½¿ç”¨ F.softmax(attn, dim=1)

    def forward(self, x):
        # xå½¢çŠ¶: (batch_size, num_nodes, in_features)
        batch_size, num_nodes, _ = x.shape

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå½¢çŠ¶å˜åŒ–ä¸º (batch_size, num_nodes, memory_size)
        attn = torch.matmul(x, self.Mk.T)  # é€šè¿‡Mkå°†è¾“å…¥æ˜ å°„åˆ°å¤–éƒ¨è®°å¿†ç©ºé—´

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œå½¢çŠ¶å˜åŒ–ä¸º (batch_size, num_nodes, memory_size)
        attn = F.softmax(attn, dim=1)

        attn = attn / torch.sum(attn, dim=2, keepdim=True)  # å½’ä¸€åŒ–

        # ä½¿ç”¨æ³¨æ„åŠ›æƒé‡èšåˆ Mvï¼Œå½¢çŠ¶å˜åŒ–ä¸º (batch_size, num_nodes, hidden_dim)
        out = torch.matmul(attn, self.Mv)  # é€šè¿‡Mvä»å¤–éƒ¨è®°å¿†ç©ºé—´é‡æ„ç‰¹å¾

        return out


class MPNNconv(nn.Module):
    def __init__(self, node_in_feats, edge_in_feats, node_out_feats,
                 edge_hidden_feats=32, num_step_message_passing=1):
        super(MPNNconv, self).__init__()

        self.project_node_feats = nn.Sequential(
            nn.Linear(node_in_feats, node_out_feats),  # (38,38)
            nn.ReLU()
        )
        self.num_step_message_passing = num_step_message_passing

        edge_network = nn.Sequential(
            nn.Linear(edge_in_feats, edge_hidden_feats),  # (1,32)
            nn.ReLU(),
            nn.Linear(edge_hidden_feats, node_out_feats * node_out_feats)  # (32,1444)
        )
        self.gnn_layer = gnn.NNConv(
            node_out_feats,
            node_out_feats,
            edge_network,
            aggr='add'
        )
        self.gru = nn.GRU(node_out_feats, node_out_feats)

    def reset_parameters(self):
        self.project_node_feats[0].reset_parameters()
        self.gnn_layer.reset_parameters()
        for layer in self.gnn_layer.edge_func:
            if isinstance(layer, nn.Linear):
                layer.reset_parameters()
        self.gru.reset_parameters()

    def forward(self, system_graph):

        node_feats = system_graph.x
        edge_index = system_graph.edge_index
        edge_feats = system_graph.edge_attr

        node_feats = self.project_node_feats(node_feats)  # (V, node_out_feats)
        hidden_feats = node_feats.unsqueeze(0)  # (1, V, node_out_feats)

        for _ in range(self.num_step_message_passing):
            if torch.cuda.is_available():
                node_feats = F.relu(self.gnn_layer(x=node_feats.type(torch.FloatTensor).cuda(),
                                                   edge_index=edge_index.type(torch.LongTensor).cuda(),
                                                   edge_attr=edge_feats.type(torch.FloatTensor).cuda()))
            else:
                node_feats = F.relu(self.gnn_layer(x=node_feats.type(torch.FloatTensor),
                                                   edge_index=edge_index.type(torch.LongTensor),
                                                   edge_attr=edge_feats.type(torch.FloatTensor)))
            node_feats, hidden_feats = self.gru(node_feats.unsqueeze(0), hidden_feats)
            node_feats = node_feats.squeeze(0)
        return node_feats

class EdgeModel(torch.nn.Module):
    def __init__(self, v_in, e_in, u_in, hidden_dim):
        super().__init__()
        self.edge_mlp = nn.Sequential(
            nn.Linear(v_in * 2 + e_in + u_in, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, src, dest, edge_attr, u, batch):
        out = torch.cat([src, dest, edge_attr, u[batch]], axis=1)
        return self.edge_mlp(out).to(device)  # æ·»åŠ  .to(device)


class NodeModel(torch.nn.Module):
    def __init__(self, v_in, u_in, hidden_dim, memory_size=128, attention_weight=1.0):
        super().__init__()
        self.ext_attention = ExternalAttentionLayer(v_in + u_in, hidden_dim, u_in, memory_size)
        # æ³¨æ„åŠ›ä½¿ç”¨æ¯”ä¾‹ï¼š1.0è¡¨ç¤ºå®Œå…¨ä½¿ç”¨æ³¨æ„åŠ›ï¼ˆåŸå§‹è¡Œä¸ºï¼‰ï¼Œ0.0-1.0ä¹‹é—´å¯ä»¥è°ƒæ•´æ³¨æ„åŠ›å½±å“
        # é€šè¿‡ç¼©æ”¾æ³¨æ„åŠ›è¾“å‡ºæ¥æ§åˆ¶å…¶å½±å“ç¨‹åº¦
        self.attention_weight = attention_weight
        # å¦‚æœattention_weight < 1.0ï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ªæŠ•å½±å±‚æ¥å°†åŸå§‹è¾“å…¥æ˜ å°„åˆ°hidden_dim
        # æ³¨æ„ï¼šcombined_inputçš„ç»´åº¦æ˜¯åŠ¨æ€çš„ï¼Œéœ€è¦åœ¨forwardä¸­å»¶è¿Ÿåˆå§‹åŒ–æŠ•å½±å±‚
        self.input_projection = None
        self._input_proj_dim = None
        self.node_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2 + u_in, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, x, edge_index, edge_attr, u, batch):
        out = scatter_add(edge_attr, edge_index[1], dim=0, dim_size=x.size(0))
        combined_input = torch.cat([x, out, u[batch]], dim=1)
        combined_dim = combined_input.size(1)

        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œå½¢çŠ¶å˜ä¸º (1, num_nodes, v_in + u_in)
        out = combined_input.unsqueeze(0)
        attn_out = self.ext_attention(out)  # æ³¨æ„åŠ›è¾“å‡º
        attn_out = attn_out.squeeze(0)  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
        
        # æ ¹æ®attention_weightè°ƒæ•´æ³¨æ„åŠ›çš„ä½¿ç”¨æ¯”ä¾‹
        # attention_weight=1.0: å®Œå…¨ä½¿ç”¨æ³¨æ„åŠ›è¾“å‡ºï¼ˆåŸå§‹è¡Œä¸ºï¼‰
        # attention_weight=0.5: æ³¨æ„åŠ›è¾“å‡ºå’ŒåŸå§‹è¾“å…¥æŠ•å½±å„å 50%
        # attention_weight=0.0: å®Œå…¨è·³è¿‡æ³¨æ„åŠ›ï¼Œåªä½¿ç”¨åŸå§‹è¾“å…¥æŠ•å½±
        if self.attention_weight >= 1.0:
            # å®Œå…¨ä½¿ç”¨æ³¨æ„åŠ›è¾“å‡ºï¼ˆåŸå§‹è¡Œä¸ºï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            out = attn_out
        else:
            # éœ€è¦æ··åˆæ³¨æ„åŠ›è¾“å‡ºå’ŒåŸå§‹è¾“å…¥æŠ•å½±
            # å»¶è¿Ÿåˆå§‹åŒ–æŠ•å½±å±‚ï¼ˆå› ä¸ºcombined_dimåœ¨è¿è¡Œæ—¶æ‰çŸ¥é“ï¼‰
            if self.input_projection is None or self._input_proj_dim != combined_dim:
                self._input_proj_dim = combined_dim
                self.input_projection = nn.Linear(combined_dim, attn_out.size(1)).to(combined_input.device)
            
            original_proj = self.input_projection(combined_input)
            
            if self.attention_weight <= 0.0:
                # å®Œå…¨è·³è¿‡æ³¨æ„åŠ›ï¼Œåªä½¿ç”¨åŸå§‹è¾“å…¥æŠ•å½±
                out = original_proj
            else:
                # åŠ æƒæ··åˆï¼šattention_weight * attn_out + (1 - attention_weight) * åŸå§‹è¾“å…¥æŠ•å½±
                out = self.attention_weight * attn_out + (1.0 - self.attention_weight) * original_proj

        return self.node_mlp(out), None

def ensure_tensor(value):
    """ç¡®ä¿å€¼æ˜¯ tensorï¼Œå¦‚æœæ˜¯ tuple åˆ™é€’å½’è§£åŒ…"""
    while isinstance(value, tuple):
        value = value[0] if len(value) > 0 else None
    if value is None:
        raise ValueError("Cannot extract tensor from tuple")
    if not isinstance(value, torch.Tensor):
        raise TypeError(f"Expected tensor, but got {type(value)}")
    return value

class GlobalModel(torch.nn.Module):
    def __init__(self, u_in, hidden_dim):
        super().__init__()
        self.global_mlp = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim + u_in, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, x, edge_index, edge_attr, u, batch):
        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯ tensor ç±»å‹ï¼Œè€Œä¸æ˜¯ tuple
        # å¦‚æœæ˜¯ tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼›å¦‚æœä»ç„¶æ˜¯ tupleï¼Œé€’å½’è§£åŒ…
        while isinstance(x, tuple):
            x = x[0]
        while isinstance(batch, tuple):
            batch = batch[0]
        while isinstance(edge_attr, tuple):
            edge_attr = edge_attr[0]
        while isinstance(u, tuple):
            u = u[0]
        while isinstance(edge_index, tuple):
            edge_index = edge_index[0]
            
        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯ tensor ç±»å‹
        if not isinstance(x, torch.Tensor):
            raise TypeError(f"Expected x to be a tensor, but got {type(x)}")
        if not isinstance(batch, torch.Tensor):
            raise TypeError(f"Expected batch to be a tensor, but got {type(batch)}")
        if not isinstance(edge_attr, torch.Tensor):
            raise TypeError(f"Expected edge_attr to be a tensor, but got {type(edge_attr)}")
        if not isinstance(u, torch.Tensor):
            raise TypeError(f"Expected u to be a tensor, but got {type(u)}")
        if not isinstance(edge_index, torch.Tensor):
            raise TypeError(f"Expected edge_index to be a tensor, but got {type(edge_index)}")
            
        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        x = x.to(device)
        batch = batch.to(device)
        edge_attr = edge_attr.to(device)
        u = u.to(device)
        edge_index = edge_index.to(device)
        
        node_aggregate = scatter_mean(x, batch, dim=0)
        edge_aggregate = scatter_mean(edge_attr, batch[edge_index[1]], dim=0,
                                      out=edge_attr.new_zeros(node_aggregate.shape))
        out = torch.cat([u, node_aggregate, edge_aggregate], dim=1)
        return self.global_mlp(out).to(device)  # æ·»åŠ  .to(device)


class GHGEAT(nn.Module):
    def __init__(self, v_in, e_in, u_in, hidden_dim, attention_weight=1.0):
        super(GHGEAT, self).__init__()
        # attention_weight: æ³¨æ„åŠ›ä½¿ç”¨æ¯”ä¾‹ï¼ŒèŒƒå›´[0.0, 1.0]
        # 1.0è¡¨ç¤ºå®Œå…¨ä½¿ç”¨æ³¨æ„åŠ›ï¼ˆåŸå§‹è¡Œä¸ºï¼‰ï¼Œ0.0è¡¨ç¤ºå®Œå…¨è·³è¿‡æ³¨æ„åŠ›
        # å¯ä»¥é€šè¿‡è°ƒæ•´æ­¤å‚æ•°æ¥æ§åˆ¶æ³¨æ„åŠ›æœºåˆ¶çš„å½±å“ç¨‹åº¦
        self.attention_weight = attention_weight

        self.graphnet1 = gnn.MetaLayer(
            EdgeModel(v_in, e_in, u_in, hidden_dim),
            NodeModel(v_in, u_in, hidden_dim, attention_weight=attention_weight),
            GlobalModel(u_in, hidden_dim)
        )
        self.graphnet2 = gnn.MetaLayer(
            EdgeModel(hidden_dim, hidden_dim, hidden_dim, hidden_dim),
            NodeModel(hidden_dim, hidden_dim, hidden_dim, attention_weight=attention_weight),
            GlobalModel(hidden_dim, hidden_dim)
        )

        self.gnorm1 = gnn.GraphNorm(hidden_dim)
        self.gnorm2 = gnn.GraphNorm(hidden_dim)

        self.pool = global_mean_pool

        self.global_conv1 = MPNNconv(node_in_feats=hidden_dim * 2,
                                     edge_in_feats=1,
                                     node_out_feats=hidden_dim * 2)

        # MLP for A
        self.mlp1a = nn.Linear(hidden_dim * 4, hidden_dim)
        self.mlp2a = nn.Linear(hidden_dim, hidden_dim)
        self.mlp3a = nn.Linear(hidden_dim, 1)

        # MLP for B
        self.mlp1b = nn.Linear(hidden_dim * 4, hidden_dim)
        self.mlp2b = nn.Linear(hidden_dim, hidden_dim)
        self.mlp3b = nn.Linear(hidden_dim, 1)
        
        # ğŸ”‘ åˆå§‹åŒ–è¾“å‡ºå±‚æƒé‡ï¼šä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–èŒƒå›´ï¼Œé¿å…åˆå§‹é¢„æµ‹å€¼è¿‡å¤§
        # è¿™æœ‰åŠ©äºé˜²æ­¢è®­ç»ƒåˆæœŸçš„æ•°å€¼ä¸ç¨³å®š
        self._initialize_output_layers()

    def _initialize_output_layers(self):
        """åˆå§‹åŒ–è¾“å‡ºå±‚æƒé‡ï¼Œä½¿ç”¨è¾ƒå°çš„èŒƒå›´ä»¥é¿å…æ•°å€¼ä¸ç¨³å®š"""
        # å¯¹è¾“å‡ºå±‚mlp3aå’Œmlp3bä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼ŒèŒƒå›´åœ¨[-0.01, 0.01]ä¹‹é—´
        # è¿™æ ·å¯ä»¥ç¡®ä¿åˆå§‹é¢„æµ‹å€¼ä¸ä¼šå¤ªå¤§
        for layer in [self.mlp3a, self.mlp3b]:
            if hasattr(layer, 'weight') and layer.weight is not None:
                nn.init.uniform_(layer.weight, -0.01, 0.01)
            if hasattr(layer, 'bias') and layer.bias is not None:
                nn.init.uniform_(layer.bias, -0.01, 0.01)

    def generate_sys_graph(self, x, edge_attr, batch_size, n_mols=2):
        src = np.arange(batch_size)
        dst = np.arange(batch_size, n_mols * batch_size)
        self_connection = np.arange(n_mols * batch_size)
        one_way = np.concatenate((src, dst, self_connection))
        other_way = np.concatenate((dst, src, self_connection))
        edge_index = torch.tensor([list(one_way), list(other_way)], dtype=torch.long).to(device)  # æ·»åŠ  .to(device)
        sys_graph = Data(x=x.to(device), edge_index=edge_index, edge_attr=edge_attr.to(device))  # æ·»åŠ  .to(device)
        return sys_graph

    def forward(self, solvent, solute, T):
        # Molecular descriptors based on MOSCED model
        ap1 = solvent.ap.reshape(-1, 1).to(device)  # æ·»åŠ  .to(device)
        ap2 = solute.ap.reshape(-1, 1).to(device)  # æ·»åŠ  .to(device)
        bp1 = solvent.bp.reshape(-1, 1).to(device)  # æ·»åŠ  .to(device)
        bp2 = solute.bp.reshape(-1, 1).to(device)  # æ·»åŠ  .to(device)
        topopsa1 = solvent.topopsa.reshape(-1, 1).to(device)  # æ·»åŠ  .to(device)
        topopsa2 = solute.topopsa.reshape(-1, 1).to(device)  # æ·»åŠ  .to(device)
        intra_hb1 = solvent.inter_hb.to(device)  # æ·»åŠ  .to(device)
        intra_hb2 = solute.inter_hb.to(device)  # æ·»åŠ  .to(device)

        u1 = torch.cat((ap1, bp1, topopsa1), axis=1).to(device)  # æ·»åŠ  .to(device)
        u2 = torch.cat((ap2, bp2, topopsa2), axis=1).to(device)  # æ·»åŠ  .to(device)

        single_node_batch = False
        if solvent.edge_attr.shape[0] == 0 or solute.edge_attr.shape[0] == 0:
            solvent = cat_dummy_graph(solvent)
            solute = cat_dummy_graph(solute)
            u1_dummy = torch.tensor([1, 1, 1]).reshape(1, -1).to(device)  # æ·»åŠ  .to(device)
            u2_dummy = torch.tensor([1, 1, 1]).reshape(1, -1).to(device)  # æ·»åŠ  .to(device)
            u1 = torch.cat((u1, u1_dummy), axis=0)
            u2 = torch.cat((u2, u2_dummy), axis=0)
            single_node_batch = True

        # Solvent GraphNet
        solvent.x = solvent.x.to(device)  # æ·»åŠ  .to(device)
        solvent.edge_index = solvent.edge_index.to(device)  # æ·»åŠ  .to(device)
        solvent.edge_attr = solvent.edge_attr.to(device)  # æ·»åŠ  .to(device)
        solvent.batch = solvent.batch.to(device)  # æ·»åŠ  .to(device)

        solute.x = solute.x.to(device)  # æ·»åŠ  .to(device)
        solute.edge_index = solute.edge_index.to(device)  # æ·»åŠ  .to(device)
        solute.edge_attr = solute.edge_attr.to(device)  # æ·»åŠ  .to(device)
        solute.batch = solute.batch.to(device)  # æ·»åŠ  .to(device)

        x1, edge_attr1, u1 = self.graphnet1(solvent.x,
                                            solvent.edge_index,
                                            solvent.edge_attr,
                                            u1,
                                            solvent.batch)
        # ç¡®ä¿è¿”å›å€¼æ˜¯ tensor è€Œä¸æ˜¯ tuple
        x1 = ensure_tensor(x1)
        edge_attr1 = ensure_tensor(edge_attr1)
        u1 = ensure_tensor(u1)
        
        x1 = self.gnorm1(x1, solvent.batch)
        x1, edge_attr1, u1 = self.graphnet2(x1,
                                            solvent.edge_index,
                                            edge_attr1,
                                            u1,
                                            solvent.batch)
        # å†æ¬¡ç¡®ä¿è¿”å›å€¼æ˜¯ tensor
        x1 = ensure_tensor(x1)
        edge_attr1 = ensure_tensor(edge_attr1)
        u1 = ensure_tensor(u1)
        
        x1 = self.gnorm2(x1, solvent.batch)
        xg1 = self.pool(x1, solvent.batch)

        # Solute GraphNet
        x2, edge_attr2, u2 = self.graphnet1(solute.x,
                                            solute.edge_index,
                                            solute.edge_attr,
                                            u2,
                                            solute.batch)
        # ç¡®ä¿è¿”å›å€¼æ˜¯ tensor
        x2 = ensure_tensor(x2)
        edge_attr2 = ensure_tensor(edge_attr2)
        u2 = ensure_tensor(u2)
        
        x2 = self.gnorm1(x2, solute.batch)
        x2, edge_attr2, u2 = self.graphnet2(x2,
                                            solute.edge_index,
                                            edge_attr2,
                                            u2,
                                            solute.batch)
        # å†æ¬¡ç¡®ä¿è¿”å›å€¼æ˜¯ tensor
        x2 = ensure_tensor(x2)
        edge_attr2 = ensure_tensor(edge_attr2)
        u2 = ensure_tensor(u2)
        
        x2 = self.gnorm2(x2, solute.batch)
        xg2 = self.pool(x2, solute.batch)

        if single_node_batch:
            xg1 = xg1[:-1, :]
            xg2 = xg2[:-1, :]
            u1 = u1[:-1, :]
            u2 = u2[:-1, :]
            solvent.inter_hb = solvent.inter_hb[:-1]
            solute.inter_hb = solute.inter_hb[:-1]
            batch_size = solvent.y.shape[0] - 1
        else:
            batch_size = solvent.y.shape[0]

        inter_hb = solvent.inter_hb
        node_feat = torch.cat((
            torch.cat((xg1, u1), axis=1),
            torch.cat((xg2, u2), axis=1)), axis=0)
        edge_feat = torch.cat((inter_hb.repeat(2),
                               intra_hb1,
                               intra_hb2)).unsqueeze(1)

        binary_sys_graph = self.generate_sys_graph(x=node_feat,
                                                   edge_attr=edge_feat,
                                                   batch_size=batch_size)

        xg = self.global_conv1(binary_sys_graph)
        xg = torch.cat((xg[0:len(xg) // 2, :], xg[len(xg) // 2:, :]), axis=1)

        T = T.x.reshape(-1, 1) + 273.15
        # åŸºæœ¬çš„æ¸©åº¦ä¿æŠ¤ï¼šç¡®ä¿æ¸©åº¦ä¸ä¼šè¿‡å°ï¼ˆé˜²æ­¢é™¤ä»¥0ï¼‰
        # æ‘„æ°åº¦è½¬å¼€å°”æ–‡ï¼šT_C + 273.15ï¼Œæ­£å¸¸èŒƒå›´åº”è¯¥åœ¨200K-600K
        # åªå¯¹æç«¯å¼‚å¸¸å€¼è¿›è¡Œä¿æŠ¤ï¼Œä½¿ç”¨ä¸€ä¸ªéå¸¸å°çš„é˜ˆå€¼ï¼ˆå¦‚10Kï¼‰
        T = torch.clamp(T, min=10.0)

        A = F.gelu(self.mlp1a(xg))
        A = F.gelu(self.mlp2a(A))
        A = self.mlp3a(A)

        B = F.gelu(self.mlp1b(xg))
        B = F.gelu(self.mlp2b(B))
        B = self.mlp3b(B)
        
        # ğŸ”‘ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤ï¼šé˜²æ­¢Bå€¼è¿‡å¤§å¯¼è‡´B/Tæº¢å‡º
        # é™åˆ¶Bçš„èŒƒå›´ï¼Œé¿å…B/Täº§ç”Ÿinfï¼ˆä¾‹å¦‚ï¼Œå¦‚æœT=273Kï¼ŒB>1e38ä¼šå¯¼è‡´æº¢å‡ºï¼‰
        # ä½¿ç”¨åˆç†çš„ç‰©ç†èŒƒå›´ï¼šé€šå¸¸Bçš„å€¼åº”è¯¥åœ¨-1e4åˆ°1e4ä¹‹é—´
        B_max = 1e4  # é˜²æ­¢B/Tæº¢å‡ºçš„æœ€å¤§Bå€¼
        B_min = -1e4  # é˜²æ­¢B/Tæº¢å‡ºçš„æœ€å°Bå€¼
        B = torch.clamp(B, min=B_min, max=B_max)
        
        # ğŸ”‘ è®¡ç®—B/Tæ—¶æ·»åŠ é¢å¤–ä¿æŠ¤ï¼šå¦‚æœB/Tå¯èƒ½æº¢å‡ºï¼Œè¿›è¡Œè£å‰ª
        # é¿å…é™¤ä»¥éå¸¸å°çš„Tå¯¼è‡´B/Tè¿‡å¤§
        B_over_T = B / T
        
        # æ£€æŸ¥B/Tæ˜¯å¦ä¼šå¯¼è‡´æ•°å€¼æº¢å‡ºï¼ˆå¦‚infæˆ–nanï¼‰
        # å¦‚æœTå¤ªå°æˆ–Bå¤ªå¤§ï¼ŒB/Tå¯èƒ½éå¸¸å¤§ï¼Œéœ€è¦è£å‰ª
        B_over_T_max = 1e6  # B/Tçš„æœ€å¤§åˆç†å€¼
        B_over_T_min = -1e6  # B/Tçš„æœ€å°åˆç†å€¼
        B_over_T = torch.clamp(B_over_T, min=B_over_T_min, max=B_over_T_max)
        
        output = A + B_over_T
        
        # ğŸ”‘ æœ€ç»ˆè¾“å‡ºä¿æŠ¤ï¼šç¡®ä¿è¾“å‡ºä¸åŒ…å«infæˆ–nan
        # å¦‚æœè¾“å‡ºåŒ…å«å¼‚å¸¸å€¼ï¼Œè¿›è¡Œè£å‰ª
        output_max = 1e6  # è¾“å‡ºçš„æœ€å¤§åˆç†å€¼
        output_min = -1e6  # è¾“å‡ºçš„æœ€å°åˆç†å€¼
        output = torch.clamp(output, min=output_min, max=output_max)
        
        # ğŸ”‘ æ£€æµ‹å¹¶å¤„ç†NaN/Infï¼šå¦‚æœä»æœ‰å¼‚å¸¸å€¼ï¼Œç”¨0æ›¿æ¢ï¼ˆä½œä¸ºæœ€åæ‰‹æ®µï¼‰
        # è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¦‚æœå‘ç”Ÿäº†ï¼Œè‡³å°‘ä¸ä¼šå¯¼è‡´è®­ç»ƒå´©æºƒ
        if torch.any(~torch.isfinite(output)):
            print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°è¾“å‡ºä¸­çš„NaN/Infï¼Œä½ç½®: {torch.where(~torch.isfinite(output))}")
            output = torch.where(torch.isfinite(output), output, torch.zeros_like(output))

        return output.to(device)  # æ·»åŠ  .to(device)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)